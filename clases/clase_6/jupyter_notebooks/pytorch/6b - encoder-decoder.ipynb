{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6b - encoder-decoder.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPvl3TaISsL3B8bhERpbYE2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pfa39F4lsLf3"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## LSTM encoder-decoder"]},{"cell_type":"markdown","metadata":{"id":"ZqO0PRcFsPTe"},"source":["### 1 - Datos\n","El objecto es utilizar una serie de sucuencias númericas (datos sintéticos) para poner a prueba el uso de las redes LSTM. Este ejemplo se inspiró en otro artículo, lo tienen como referencia en el siguiente link:\\\n","[LINK](https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras-part-2/)"]},{"cell_type":"code","metadata":{"id":"cq3YXak9sGHd","executionInfo":{"status":"ok","timestamp":1655122646442,"user_tz":180,"elapsed":487,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}}},"source":["import re\n","\n","import numpy as np\n","import pandas as pd\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader"],"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# torchsummar actualmente tiene un problema con las LSTM, por eso\n","# se utiliza torchinfo, un fork del proyecto original con el bug solucionado\n","!pip3 install torchinfo\n","from torchinfo import summary"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"djTjSRzum91t","executionInfo":{"status":"ok","timestamp":1655122650076,"user_tz":180,"elapsed":3235,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"d91d7faa-fc82-4f3b-be6d-3167898772ae"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.7.0)\n"]}]},{"cell_type":"code","source":["import os\n","import platform\n","\n","if os.access('torch_helpers.py', os.F_OK) is False:\n","    if platform.system() == 'Windows':\n","        !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py\n","    else:\n","        !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py"],"metadata":{"id":"QEG1S1WPm_aI","executionInfo":{"status":"ok","timestamp":1655122650077,"user_tz":180,"elapsed":29,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["def to_categorical(y, num_classes=None, dtype='float32'):\n","    y = np.array(y, dtype='int')\n","    input_shape = y.shape\n","    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n","        input_shape = tuple(input_shape[:-1])\n","    y = y.ravel()\n","    if not num_classes:\n","        num_classes = np.max(y) + 1\n","    n = y.shape[0]\n","    categorical = np.zeros((n, num_classes), dtype=dtype)\n","    categorical[np.arange(n), y] = 1\n","    output_shape = input_shape + (num_classes,)\n","    categorical = np.reshape(categorical, output_shape)\n","    return categorical"],"metadata":{"id":"dIb2pVygsqBN","executionInfo":{"status":"ok","timestamp":1655122650077,"user_tz":180,"elapsed":27,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["### 1 - Datos"],"metadata":{"id":"iO490AXPnC0t"}},{"cell_type":"code","metadata":{"id":"-9aNLZBDtA5J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655122650078,"user_tz":180,"elapsed":26,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"b3009506-15ab-4b11-9d10-246b8d3fab7e"},"source":["# Generar datos sintéticos\n","X = list()\n","y = list()\n","\n","# En ambos casos \"X\" e \"y\" son vectores de números de 5 en 5\n","X = [x for x in range(5, 301, 5)]\n","y = [x+15 for x in X]\n","\n","print(f\"datos X (len={len(X)}):\", X)\n","print(f\"datos y (len={len(y)}):\", y)"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["datos X (len=60): [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300]\n","datos y (len=60): [20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300, 305, 310, 315]\n"]}]},{"cell_type":"code","metadata":{"id":"5ACkrI0GtEAM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655122650078,"user_tz":180,"elapsed":19,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"65dafa8f-5de5-4d70-ae56-047826348f9b"},"source":["# Se desea agrupar los datos de a 3 elementos\n","X = np.array(X).reshape(len(X)//3, 3, 1)\n","y = np.array(y).reshape(len(y)//3, 3, 1)\n","print(\"datos X[0:2]:\", X[0:2])\n","print(\"datos y[0:2]:\", y[0:2])"],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["datos X[0:2]: [[[ 5]\n","  [10]\n","  [15]]\n","\n"," [[20]\n","  [25]\n","  [30]]]\n","datos y[0:2]: [[[20]\n","  [25]\n","  [30]]\n","\n"," [[35]\n","  [40]\n","  [45]]]\n"]}]},{"cell_type":"code","metadata":{"id":"ajykLcJ8tFfN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655122650079,"user_tz":180,"elapsed":14,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"7088d42c-607c-4638-f37a-61039ae0ce29"},"source":["# Verificamos que la secuencia enetrada es igual a la secuencia de salida\n","# en cuanto a dimensiones\n","print(\"X shape:\", X.shape)\n","print(\"y shape:\", y.shape)"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["X shape: (20, 3, 1)\n","y shape: (20, 3, 1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"LZz9Zsvy5ilc"},"source":["### 2 - Preprocesamiento"]},{"cell_type":"code","metadata":{"id":"J86H4qb6NEQp","executionInfo":{"status":"ok","timestamp":1655122650768,"user_tz":180,"elapsed":700,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}}},"source":["from sklearn import preprocessing\n","\n","# Preparar datos para consumir por la layers LSTM\n","# Primero se toman todos los valores posibles que pueden tomar X e Y\n","# Con eso se genera un LabelEncoder para transformar de 0 - n_features\n","# Por cada dato de X e Y se genera su contraparte oneHotEncoding\n","# X1 equivale a X, es la secuencia de entrada\n","# X2 equivale a y sin el ultimo elemento, equivale a la secuenca \"estado anterior\"\n","# target equivale a Y como OneHotEncoding, la prediccion completa con el ultimo elemento\n","\n","def get_dataset(X, Y, label_encoder=None):\n","    data = np.append(X, Y)\n","    labels = np.unique(data)\n","\n","    if label_encoder is None:\n","        label_encoder = preprocessing.LabelEncoder()\n","        label_encoder.fit(labels)\n","\n","    cardinality = len(label_encoder.classes_)\n","    print(\"Number of features/cardinality:\", cardinality)\n","\n","    X1, X2, target = list(), list(), list()\n","    for x, y in zip(X, Y):\n","        input = list(label_encoder.transform(x.reshape(-1)))\n","        output = list(label_encoder.transform(y.reshape(-1)))\n","        # Crear la entrada del \"ultimo estado\" de salida\n","        # que es la salida sin el ultimo elemento, que es el que el modelo\n","        # debe predecir\n","        output_in = [0] + output[:-1]\n","\n","        # transformar\n","        input_encoded = to_categorical(input, num_classes=cardinality, dtype=\"int32\")\n","        output_encoded = to_categorical(output, num_classes=cardinality, dtype=\"int32\")\n","        output_in_encoded = to_categorical(output_in, num_classes=cardinality, dtype=\"int32\")\n","       \n","        # almacenar\n","        X1.append(input_encoded)\n","        X2.append(output_in_encoded)\n","        target.append(output_encoded)\n","    return np.array(X1), np.array(X2), np.array(target), label_encoder"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"pO2sS3ntkeEf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655122650769,"user_tz":180,"elapsed":11,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"19352575-d9ff-40a9-b671-83a0d9947f86"},"source":["X1, X2, target, label_encoder = get_dataset(X, y)"],"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of features/cardinality: 63\n"]}]},{"cell_type":"code","source":["class Data(Dataset):\n","    def __init__(self, encoder_inputs, decoder_inputs, decoder_outputs):\n","        self.encoder_inputs = torch.from_numpy(encoder_inputs.astype(np.int32)).float()\n","        self.decoder_inputs = torch.from_numpy(decoder_inputs.astype(np.int32)).float()\n","        self.decoder_outputs = torch.from_numpy(decoder_outputs.astype(np.int32)).float()\n","\n","        self.len = self.decoder_outputs.shape[0]\n","\n","    def __getitem__(self,index):\n","        return self.encoder_inputs[index], self.decoder_inputs[index], self.decoder_outputs[index]\n","\n","    def __len__(self):\n","        return self.len\n","\n","data_set = Data(X1, X2, target)\n","\n","# El vector de salida tiene 3 dimensiones:\n","# primera dimension es la cantidad de \"rows\" del dataset\n","# segunda dimension es el tamaño de la sequencia de entrada/salida\n","# tercera dimensión es la dimensionalidad del vector oneHotEncoding (cardinality)\n","print(\"X1 shape:\", X1.shape)\n","print(\"X2 shape:\", X2.shape)\n","print(\"target shape:\", target.shape)\n","\n","encoder_input_size = data_set.encoder_inputs.shape[-1]\n","print(\"encoder_input_size:\", encoder_input_size)\n","\n","decoder_input_size = data_set.decoder_inputs.shape[-1]\n","print(\"decoder_input_size:\", decoder_input_size)\n","\n","output_dim = data_set.decoder_outputs.shape[-1]\n","print(\"Output dim\", output_dim)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hfBkq-YeoHRu","executionInfo":{"status":"ok","timestamp":1655122650769,"user_tz":180,"elapsed":10,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"72f191a8-5455-487a-e37d-e84408bc654d"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["X1 shape: (20, 3, 63)\n","X2 shape: (20, 3, 63)\n","target shape: (20, 3, 63)\n","encoder_input_size: 63\n","decoder_input_size: 63\n","Output dim 63\n"]}]},{"cell_type":"code","source":["torch.manual_seed(42)\n","valid_set_size = int(data_set.len * 0.2)\n","train_set_size = data_set.len - valid_set_size\n","\n","train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n","valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len))\n","\n","print(\"Tamaño del conjunto de entrenamiento:\", len(train_set))\n","print(\"Tamaño del conjunto de validacion:\", len(valid_set))\n","\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u9UfRiuVpoU6","executionInfo":{"status":"ok","timestamp":1655122650769,"user_tz":180,"elapsed":8,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"514e5f89-8e41-4770-9440-218ac5db3e40"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Tamaño del conjunto de entrenamiento: 16\n","Tamaño del conjunto de validacion: 4\n"]}]},{"cell_type":"markdown","metadata":{"id":"3vKbhjtIwPgM"},"source":["### 3 - Entrenar el modelo"]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim):\n","        super().__init__()\n","        self.lstm_size = 128\n","        self.num_layers = 1\n","\n","        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=self.lstm_size, batch_first=True,\n","                            num_layers=self.num_layers) # LSTM layer\n","\n","    def forward(self, x):\n","        lstm_output, (ht, ct) = self.lstm(x)\n","        return (ht, ct)\n","\n","class Decoder(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        self.lstm_size = 128\n","        self.num_layers = 1\n","        self.output_dim = output_dim\n","\n","        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=self.lstm_size, batch_first=True,\n","                            num_layers=self.num_layers) # LSTM layer\n","        self.fc1 = nn.Linear(in_features=self.lstm_size, out_features=self.output_dim) # Fully connected layer\n","\n","        self.softmax = nn.Softmax(dim=1) # normalize in dim 1\n","\n","    def forward(self, x, prev_state):\n","        lstm_output, (ht, ct) = self.lstm(x, prev_state)\n","        out = self.softmax(self.fc1(lstm_output[:,-1,:])) # take last output (last seq)\n","        return out, (ht, ct)\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        \n","        assert encoder.lstm_size == decoder.lstm_size, \\\n","            \"Hidden dimensions of encoder and decoder must be equal!\"\n","        assert encoder.num_layers == decoder.num_layers, \\\n","            \"Encoder and decoder must have equal number of layers!\"\n","        \n","    def forward(self, encoder_input, decoder_input):\n","        batch_size = decoder_input.shape[0]\n","        decoder_input_len = decoder_input.shape[1]\n","        vocab_size = self.decoder.output_dim\n","        \n","        # tensor para almacenar la salida\n","        # (batch_size, sentence_len, one_hot_size)\n","        outputs = torch.zeros(batch_size, decoder_input_len, vocab_size)\n","        \n","        # ultimo hidden state del encoder, primer estado oculto del decoder\n","        prev_state = self.encoder(encoder_input)\n","      \n","        # En la primera iteracion se toma el primer token de target (<sos>)\n","        input = decoder_input[:, 0:1]\n","\n","        for t in range(decoder_input_len):\n","            # t --> token index\n","\n","            # utilizamos método \"teacher forcing\", es decir que durante\n","            # el entrenamiento no realimentamos la salida del decoder\n","            # sino el token correcto que sigue en target\n","            input = decoder_input[:, t:t+1]\n","\n","            # ingresar cada token embedding, uno por uno junto al hidden state\n","            # recibir el output del decoder (softmax)\n","            output, prev_state = self.decoder(input, prev_state)\n","            top1 = output.argmax(1).view(-1, 1)\n","\n","            # Sino se usará \"teacher forcing\" habría que descomentar\n","            # esta linea.\n","            # Hay ejemplos dandos vuelta en donde se utilza un random \n","            # para ver en cada vuelta que técnica se aplica\n","            #input = top1            \n","\n","            # guardar cada salida (softmax)\n","            outputs[:, t, :] = output\n","\n","        return outputs\n","\n","encoder = Encoder(input_dim=encoder_input_size)\n","# decoder --> input_dim == output_dim --> porque recibe y devuelve en el mismo vocabulario\n","decoder = Decoder(input_dim=decoder_input_size, output_dim=output_dim)\n","\n","model = Seq2Seq(encoder, decoder)\n","\n","# Crear el optimizador la una función de error\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","#criterion = torch.nn.MSELoss()  # Para clasificación multi categórica\n","criterion = torch.nn.CrossEntropyLoss()  # Para clasificación multi categórica\n","\n","# Por defecto torchinfo testea el modelo con torch.FloatTensor\n","summary(model, input_data=(data_set[0:1][0], data_set[0:1][1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IW4fV1MIpzCc","executionInfo":{"status":"ok","timestamp":1655122650774,"user_tz":180,"elapsed":12,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"f79019e9-320f-47f8-e3e3-a97402967372"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","Seq2Seq                                  [1, 3, 63]                --\n","├─Encoder: 1-1                           [1, 1, 128]               --\n","│    └─LSTM: 2-1                         [1, 3, 128]               98,816\n","├─Decoder: 1-2                           [1, 63]                   --\n","│    └─LSTM: 2-2                         [1, 1, 128]               98,816\n","│    └─Linear: 2-3                       [1, 63]                   8,127\n","│    └─Softmax: 2-4                      [1, 63]                   --\n","├─Decoder: 1-3                           [1, 63]                   (recursive)\n","│    └─LSTM: 2-5                         [1, 1, 128]               (recursive)\n","│    └─Linear: 2-6                       [1, 63]                   (recursive)\n","│    └─Softmax: 2-7                      [1, 63]                   --\n","├─Decoder: 1-4                           [1, 63]                   (recursive)\n","│    └─LSTM: 2-8                         [1, 1, 128]               (recursive)\n","│    └─Linear: 2-9                       [1, 63]                   (recursive)\n","│    └─Softmax: 2-10                     [1, 63]                   --\n","==========================================================================================\n","Total params: 205,759\n","Trainable params: 205,759\n","Non-trainable params: 0\n","Total mult-adds (M): 0.62\n","==========================================================================================\n","Input size (MB): 0.03\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.82\n","Estimated Total Size (MB): 0.86\n","=========================================================================================="]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["def sequence_acc(y_pred, y_test):\n","    y_pred_tag = y_pred.data.max(dim=-1,keepdim=True)[1]\n","    y_test_tag = y_test.data.max(dim=-1,keepdim=True)[1]\n","\n","    batch_size = y_pred_tag.shape[0]\n","    batch_acc = torch.zeros(batch_size)\n","    for b in range(batch_size):\n","        correct_results_sum = (y_pred_tag[b] == y_test_tag[b]).sum().float()\n","        batch_acc[b] = correct_results_sum / y_pred_tag[b].shape[0]\n","\n","    correct_results_sum = batch_acc.sum().float()\n","    acc = correct_results_sum / batch_size\n","    return acc\n","\n","def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100):\n","    # Defino listas para realizar graficas de los resultados\n","    train_loss = []\n","    train_accuracy = []\n","    valid_loss = []\n","    valid_accuracy = []\n","\n","    # Defino mi loop de entrenamiento\n","\n","    for epoch in range(epochs):\n","\n","        epoch_train_loss = 0.0\n","        epoch_train_accuracy = 0.0\n","\n","        for train_encoder_input, train_decoder_input, train_target in train_loader:\n","            # Seteo los gradientes en cero ya que, por defecto, PyTorch\n","            # los va acumulando\n","            optimizer.zero_grad()\n","\n","            output = model(train_encoder_input, train_decoder_input)\n","\n","            # Computo el error de la salida comparando contra las etiquetas\n","            # por cada token en cada batch (sequence_loss)\n","            loss = 0\n","            for t in range(train_decoder_input.shape[1]):\n","                loss += criterion(output[:, t, :], train_target[:, t, :])\n","\n","            # Almaceno el error del batch para luego tener el error promedio de la epoca\n","            epoch_train_loss += loss.item()\n","\n","            # Computo el nuevo set de gradientes a lo largo de toda la red\n","            loss.backward()\n","\n","            # Realizo el paso de optimizacion actualizando los parametros de toda la red\n","            optimizer.step()\n","\n","            # Calculo el accuracy del batch\n","            accuracy = sequence_acc(output, train_target)\n","            # Almaceno el accuracy del batch para luego tener el accuracy promedio de la epoca\n","            epoch_train_accuracy += accuracy.item()\n","\n","        # Calculo la media de error para la epoca de entrenamiento.\n","        # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca.\n","        epoch_train_loss = epoch_train_loss / len(train_loader)\n","        train_loss.append(epoch_train_loss)\n","        epoch_train_accuracy = epoch_train_accuracy / len(train_loader)        \n","        train_accuracy.append(epoch_train_accuracy)\n","\n","        # Realizo el paso de validación computando error y accuracy, y\n","        # almacenando los valores para imprimirlos y graficarlos\n","        valid_encoder_input, valid_decoder_input, valid_target = iter(valid_loader).next()\n","        output = model(valid_encoder_input, valid_decoder_input)\n","        \n","        epoch_valid_loss = 0\n","        for t in range(train_decoder_input.shape[1]):\n","                epoch_valid_loss += criterion(output[:, t, :], valid_target[:, t, :])\n","        epoch_valid_loss = epoch_valid_loss.item()\n","\n","        valid_loss.append(epoch_valid_loss)\n","\n","        # Calculo el accuracy de la epoch\n","        epoch_valid_accuracy = sequence_acc(output, valid_target).item()\n","        valid_accuracy.append(epoch_valid_accuracy)\n","\n","        print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Train accuracy {epoch_train_accuracy:.3f} - Valid Loss {epoch_valid_loss:.3f} - Valid accuracy {epoch_valid_accuracy:.3f}\")\n","\n","    history = {\n","        \"loss\": train_loss,\n","        \"accuracy\": train_accuracy,\n","        \"val_loss\": valid_loss,\n","        \"val_accuracy\": valid_accuracy,\n","    }\n","    return history"],"metadata":{"id":"KKyhHuj9ucZX","executionInfo":{"status":"ok","timestamp":1655122650775,"user_tz":180,"elapsed":11,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["history1 = train(model,\n","                train_loader,\n","                valid_loader,\n","                optimizer,\n","                criterion,\n","                epochs=100\n","                )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8LYhz9j0uXJe","executionInfo":{"status":"ok","timestamp":1655122652402,"user_tz":180,"elapsed":1638,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"53ff153b-b7cd-4acd-b435-f50fd0ebbd5c"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/100 - Train loss 12.429 - Train accuracy 0.021 - Valid Loss 12.431 - Valid accuracy 0.000\n","Epoch: 2/100 - Train loss 12.429 - Train accuracy 0.042 - Valid Loss 12.432 - Valid accuracy 0.000\n","Epoch: 3/100 - Train loss 12.428 - Train accuracy 0.083 - Valid Loss 12.432 - Valid accuracy 0.000\n","Epoch: 4/100 - Train loss 12.428 - Train accuracy 0.104 - Valid Loss 12.432 - Valid accuracy 0.000\n","Epoch: 5/100 - Train loss 12.428 - Train accuracy 0.104 - Valid Loss 12.433 - Valid accuracy 0.000\n","Epoch: 6/100 - Train loss 12.427 - Train accuracy 0.104 - Valid Loss 12.433 - Valid accuracy 0.000\n","Epoch: 7/100 - Train loss 12.427 - Train accuracy 0.125 - Valid Loss 12.434 - Valid accuracy 0.000\n","Epoch: 8/100 - Train loss 12.427 - Train accuracy 0.146 - Valid Loss 12.434 - Valid accuracy 0.000\n","Epoch: 9/100 - Train loss 12.426 - Train accuracy 0.167 - Valid Loss 12.434 - Valid accuracy 0.000\n","Epoch: 10/100 - Train loss 12.426 - Train accuracy 0.188 - Valid Loss 12.435 - Valid accuracy 0.000\n","Epoch: 11/100 - Train loss 12.425 - Train accuracy 0.188 - Valid Loss 12.435 - Valid accuracy 0.000\n","Epoch: 12/100 - Train loss 12.425 - Train accuracy 0.167 - Valid Loss 12.436 - Valid accuracy 0.000\n","Epoch: 13/100 - Train loss 12.425 - Train accuracy 0.167 - Valid Loss 12.437 - Valid accuracy 0.000\n","Epoch: 14/100 - Train loss 12.424 - Train accuracy 0.167 - Valid Loss 12.437 - Valid accuracy 0.000\n","Epoch: 15/100 - Train loss 12.423 - Train accuracy 0.188 - Valid Loss 12.438 - Valid accuracy 0.000\n","Epoch: 16/100 - Train loss 12.423 - Train accuracy 0.188 - Valid Loss 12.439 - Valid accuracy 0.000\n","Epoch: 17/100 - Train loss 12.422 - Train accuracy 0.208 - Valid Loss 12.440 - Valid accuracy 0.000\n","Epoch: 18/100 - Train loss 12.421 - Train accuracy 0.188 - Valid Loss 12.441 - Valid accuracy 0.000\n","Epoch: 19/100 - Train loss 12.420 - Train accuracy 0.167 - Valid Loss 12.443 - Valid accuracy 0.000\n","Epoch: 20/100 - Train loss 12.419 - Train accuracy 0.167 - Valid Loss 12.444 - Valid accuracy 0.000\n","Epoch: 21/100 - Train loss 12.418 - Train accuracy 0.146 - Valid Loss 12.446 - Valid accuracy 0.000\n","Epoch: 22/100 - Train loss 12.416 - Train accuracy 0.146 - Valid Loss 12.448 - Valid accuracy 0.000\n","Epoch: 23/100 - Train loss 12.414 - Train accuracy 0.146 - Valid Loss 12.450 - Valid accuracy 0.000\n","Epoch: 24/100 - Train loss 12.412 - Train accuracy 0.146 - Valid Loss 12.452 - Valid accuracy 0.000\n","Epoch: 25/100 - Train loss 12.410 - Train accuracy 0.083 - Valid Loss 12.455 - Valid accuracy 0.000\n","Epoch: 26/100 - Train loss 12.407 - Train accuracy 0.083 - Valid Loss 12.457 - Valid accuracy 0.000\n","Epoch: 27/100 - Train loss 12.403 - Train accuracy 0.083 - Valid Loss 12.460 - Valid accuracy 0.000\n","Epoch: 28/100 - Train loss 12.400 - Train accuracy 0.083 - Valid Loss 12.462 - Valid accuracy 0.000\n","Epoch: 29/100 - Train loss 12.396 - Train accuracy 0.083 - Valid Loss 12.465 - Valid accuracy 0.000\n","Epoch: 30/100 - Train loss 12.391 - Train accuracy 0.083 - Valid Loss 12.467 - Valid accuracy 0.000\n","Epoch: 31/100 - Train loss 12.387 - Train accuracy 0.083 - Valid Loss 12.469 - Valid accuracy 0.000\n","Epoch: 32/100 - Train loss 12.383 - Train accuracy 0.062 - Valid Loss 12.471 - Valid accuracy 0.000\n","Epoch: 33/100 - Train loss 12.380 - Train accuracy 0.042 - Valid Loss 12.472 - Valid accuracy 0.000\n","Epoch: 34/100 - Train loss 12.377 - Train accuracy 0.042 - Valid Loss 12.473 - Valid accuracy 0.000\n","Epoch: 35/100 - Train loss 12.374 - Train accuracy 0.042 - Valid Loss 12.474 - Valid accuracy 0.000\n","Epoch: 36/100 - Train loss 12.370 - Train accuracy 0.042 - Valid Loss 12.474 - Valid accuracy 0.000\n","Epoch: 37/100 - Train loss 12.366 - Train accuracy 0.042 - Valid Loss 12.475 - Valid accuracy 0.000\n","Epoch: 38/100 - Train loss 12.362 - Train accuracy 0.042 - Valid Loss 12.475 - Valid accuracy 0.000\n","Epoch: 39/100 - Train loss 12.357 - Train accuracy 0.042 - Valid Loss 12.475 - Valid accuracy 0.000\n","Epoch: 40/100 - Train loss 12.352 - Train accuracy 0.042 - Valid Loss 12.475 - Valid accuracy 0.000\n","Epoch: 41/100 - Train loss 12.346 - Train accuracy 0.042 - Valid Loss 12.476 - Valid accuracy 0.000\n","Epoch: 42/100 - Train loss 12.341 - Train accuracy 0.042 - Valid Loss 12.476 - Valid accuracy 0.000\n","Epoch: 43/100 - Train loss 12.335 - Train accuracy 0.042 - Valid Loss 12.476 - Valid accuracy 0.000\n","Epoch: 44/100 - Train loss 12.328 - Train accuracy 0.062 - Valid Loss 12.476 - Valid accuracy 0.000\n","Epoch: 45/100 - Train loss 12.321 - Train accuracy 0.062 - Valid Loss 12.476 - Valid accuracy 0.000\n","Epoch: 46/100 - Train loss 12.314 - Train accuracy 0.104 - Valid Loss 12.476 - Valid accuracy 0.000\n","Epoch: 47/100 - Train loss 12.306 - Train accuracy 0.125 - Valid Loss 12.477 - Valid accuracy 0.000\n","Epoch: 48/100 - Train loss 12.297 - Train accuracy 0.125 - Valid Loss 12.477 - Valid accuracy 0.000\n","Epoch: 49/100 - Train loss 12.288 - Train accuracy 0.125 - Valid Loss 12.477 - Valid accuracy 0.000\n","Epoch: 50/100 - Train loss 12.277 - Train accuracy 0.125 - Valid Loss 12.477 - Valid accuracy 0.000\n","Epoch: 51/100 - Train loss 12.265 - Train accuracy 0.125 - Valid Loss 12.477 - Valid accuracy 0.000\n","Epoch: 52/100 - Train loss 12.252 - Train accuracy 0.146 - Valid Loss 12.477 - Valid accuracy 0.000\n","Epoch: 53/100 - Train loss 12.237 - Train accuracy 0.146 - Valid Loss 12.477 - Valid accuracy 0.000\n","Epoch: 54/100 - Train loss 12.221 - Train accuracy 0.167 - Valid Loss 12.477 - Valid accuracy 0.000\n","Epoch: 55/100 - Train loss 12.202 - Train accuracy 0.167 - Valid Loss 12.477 - Valid accuracy 0.000\n","Epoch: 56/100 - Train loss 12.182 - Train accuracy 0.167 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 57/100 - Train loss 12.159 - Train accuracy 0.167 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 58/100 - Train loss 12.134 - Train accuracy 0.188 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 59/100 - Train loss 12.107 - Train accuracy 0.208 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 60/100 - Train loss 12.077 - Train accuracy 0.229 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 61/100 - Train loss 12.046 - Train accuracy 0.271 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 62/100 - Train loss 12.012 - Train accuracy 0.271 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 63/100 - Train loss 11.975 - Train accuracy 0.271 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 64/100 - Train loss 11.936 - Train accuracy 0.333 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 65/100 - Train loss 11.893 - Train accuracy 0.417 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 66/100 - Train loss 11.846 - Train accuracy 0.437 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 67/100 - Train loss 11.796 - Train accuracy 0.521 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 68/100 - Train loss 11.742 - Train accuracy 0.521 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 69/100 - Train loss 11.685 - Train accuracy 0.562 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 70/100 - Train loss 11.626 - Train accuracy 0.583 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 71/100 - Train loss 11.563 - Train accuracy 0.583 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 72/100 - Train loss 11.497 - Train accuracy 0.646 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 73/100 - Train loss 11.429 - Train accuracy 0.667 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 74/100 - Train loss 11.360 - Train accuracy 0.688 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 75/100 - Train loss 11.288 - Train accuracy 0.708 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 76/100 - Train loss 11.217 - Train accuracy 0.729 - Valid Loss 12.478 - Valid accuracy 0.000\n","Epoch: 77/100 - Train loss 11.147 - Train accuracy 0.729 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 78/100 - Train loss 11.077 - Train accuracy 0.729 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 79/100 - Train loss 11.009 - Train accuracy 0.750 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 80/100 - Train loss 10.941 - Train accuracy 0.750 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 81/100 - Train loss 10.874 - Train accuracy 0.750 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 82/100 - Train loss 10.808 - Train accuracy 0.750 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 83/100 - Train loss 10.744 - Train accuracy 0.750 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 84/100 - Train loss 10.681 - Train accuracy 0.750 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 85/100 - Train loss 10.620 - Train accuracy 0.771 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 86/100 - Train loss 10.561 - Train accuracy 0.771 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 87/100 - Train loss 10.504 - Train accuracy 0.833 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 88/100 - Train loss 10.448 - Train accuracy 0.875 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 89/100 - Train loss 10.392 - Train accuracy 0.896 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 90/100 - Train loss 10.338 - Train accuracy 0.896 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 91/100 - Train loss 10.285 - Train accuracy 0.896 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 92/100 - Train loss 10.235 - Train accuracy 0.896 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 93/100 - Train loss 10.186 - Train accuracy 0.896 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 94/100 - Train loss 10.140 - Train accuracy 0.917 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 95/100 - Train loss 10.096 - Train accuracy 0.938 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 96/100 - Train loss 10.053 - Train accuracy 0.979 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 97/100 - Train loss 10.013 - Train accuracy 0.979 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 98/100 - Train loss 9.975 - Train accuracy 0.979 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 99/100 - Train loss 9.940 - Train accuracy 0.979 - Valid Loss 12.479 - Valid accuracy 0.000\n","Epoch: 100/100 - Train loss 9.907 - Train accuracy 0.979 - Valid Loss 12.479 - Valid accuracy 0.000\n"]}]},{"cell_type":"code","metadata":{"id":"OVz1uug_zu2J","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1655122652402,"user_tz":180,"elapsed":20,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"2b834e48-8f1f-4960-ff5a-cbeb423f461a"},"source":["epoch_count = range(1, len(history1['accuracy']) + 1)\n","sns.lineplot(x=epoch_count,  y=history1['accuracy'], label='train')\n","sns.lineplot(x=epoch_count,  y=history1['val_accuracy'], label='valid')\n","plt.show()"],"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["El sistema hizo un claro overfitting, lo cual es entendible por el tamaño del dataset y las pocas muestras de validación."],"metadata":{"id":"1uRo1Qify1cE"}},{"cell_type":"code","metadata":{"id":"71XeCtfYmOFx","executionInfo":{"status":"ok","timestamp":1655122652403,"user_tz":180,"elapsed":18,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}}},"source":["def one_hot_decode(encoded_seq, label_encoder):\n","    idx = [np.argmax(vector) for vector in encoded_seq]\n","    return label_encoder.inverse_transform(idx)"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"lR1gKJN2m2Fw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655122652403,"user_tz":180,"elapsed":17,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"c5057967-4146-473e-f1b0-02cf71f6ba45"},"source":["# Ensayo\n","x_test = [20, 25, 30]\n","y_test = [x+15 for x in x_test]\n","\n","print(\"y_test:\", y_test)\n","\n","# Transformar los datos a oneHotEncoding\n","X1_test, X2_test, target_test, _ = get_dataset(np.array([x_test]), np.array([y_test]), label_encoder)\n","\n","print(\"X1_test shape:\", X1_test.shape)\n","print(\"X2_test shape:\", X2_test.shape)\n","print(\"target_test shape:\", target_test.shape)"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["y_test: [35, 40, 45]\n","Number of features/cardinality: 63\n","X1_test shape: (1, 3, 63)\n","X2_test shape: (1, 3, 63)\n","target_test shape: (1, 3, 63)\n"]}]},{"cell_type":"code","metadata":{"id":"kluBZJGMm3LF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655122652404,"user_tz":180,"elapsed":14,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"e96fa262-9d00-4943-a1da-c6f9dc7dcb81"},"source":["# Cuando quiera por ejemplo recuperar target_test a y_test utilizo:\n","one_hot_decode(target_test[0], label_encoder)"],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([35, 40, 45])"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"C4_UQshIzw7o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655122652404,"user_tz":180,"elapsed":12,"user":{"displayName":"Hernán Contigiani","userId":"01142101934719343059"}},"outputId":"f27e9ee8-4c68-4d82-9189-252e63847823"},"source":["cardinality = len(label_encoder.classes_)\n","\n","# encode\n","# Se transforma la sequencia de entrada a los stados \"h\" y \"c\" de la LSTM\n","# para enviar la primera vez al decoder\n","X1_test_tensor = torch.from_numpy(X1_test.astype(np.int32)).float()\n","prev_state = model.encoder(X1_test_tensor)\n","\n","# start of sequence input --> la primera secuencia de salida-entrada (output_in)\n","# comienza en cero\n","target_seq = np.array([0.0 for _ in range(cardinality)]).reshape(1, 1, cardinality)\n","target_seq_tensor = torch.from_numpy(target_seq.astype(np.int32)).float()\n","print(\"target_seq shape:\", target_seq_tensor.shape)\n","\n","# Vector de predicción\n","output = list()\n","\n","for t in range(3):\n","    # Predicción del próximo elemento\n","    y_hat, new_prev_state = model.decoder(target_seq_tensor, prev_state)\n","    \n","    # Almacenar la predicción\n","    output.append(y_hat.detach().numpy()[0])\n","    \n","    # Actualizar los estados dado la ultimo prediccion\n","    prev_state = new_prev_state\n","    \n","    # Actualizar secuencia de entrada con la salida (re-alimentacion)\n","    target_seq_tensor = y_hat.unsqueeze(0)\n","\n","print(\"y_test:\", y_test)\n","print(\"y_hat:\", one_hot_decode(output, label_encoder))"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["target_seq shape: torch.Size([1, 1, 63])\n","y_test: [35, 40, 45]\n","y_hat: [35 40 45]\n"]}]},{"cell_type":"markdown","metadata":{"id":"pkOjSJweqdF8"},"source":["### 4 - Conclusión\n","A primera vista parece muy compleja la estructura del encoder-decoder, pero funciona igual en cualquier disciplina de deeplearning:\n","- En visión para transferencia de estilo, generación de imagenes, etc.\n","- En NLP desde LSTM hasta Attention y transformes\n","\n","Hay que pensar el encoder como el generador del \"espacio latente\". Luego el decoder necesita el espacio latente que representa a la setencia de entrada, la realimentación de los valores de salida del decoder y los estados internos de la LSTM para pasar a la siguiente inferencia hasta concluir la secuencia de salida."]}]}