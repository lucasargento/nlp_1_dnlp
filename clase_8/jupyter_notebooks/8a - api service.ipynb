{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBlMu5M6zSqQ"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Ejemplo de API con BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raYbCSjozZyF"
      },
      "source": [
        "## 1 - Instalar dependencias (ya sea en el colab o en su PC/servidor)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6fTperCzXk3"
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d2LXEsQ0K6E"
      },
      "source": [
        "# Descargar los pesos entrenados de BERT desde un gogle drive (es la forma más rápida)\n",
        "# NOTA: No hay garantía de que estos links perduren, en caso de que no estén\n",
        "# disponibles, se pueden obtener del entrenamiento de BERT de la clase anterior\n",
        "!curl -L -o 'bert_weights.h5' 'https://drive.google.com/u/0/uc?id=1ILoVmLK3IFMOZiWEkqvqSmnHF7a--3h2&export=download&confirm=t'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or5cDbBWypnT"
      },
      "source": [
        "%%writefile app.py\n",
        "# -------------------------------------------------------------------\n",
        "# TODO ESTO PODRÍA ESTAR EN OTRO ARCHIVO\n",
        "# ------------------------------------------------------------------\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from transformers import file_utils\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "print(\"Donde se almacena el cache?:\", file_utils.default_cache_path)\n",
        "\n",
        "\n",
        "class MyBertModel():\n",
        "    def __init__(self, model_weights_path):\n",
        "        self.max_length = 140\n",
        "        self.class_names = ['negative', 'neutral', 'positive']\n",
        "        self.model_weights_path = model_weights_path\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        output_shape = len(self.class_names)       \n",
        "\n",
        "        bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "        input_ids = tf.keras.layers.Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids')\n",
        "        attention_mask = tf.keras.layers.Input((self.max_length,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "        # Get the pooled_output (embedding que representa toda la entrada)\n",
        "        output = bert_model([input_ids, attention_mask])[1] \n",
        "\n",
        "        # We can also add dropout as regularization technique:\n",
        "        output = tf.keras.layers.Dropout(rate=0.2)(output)\n",
        "\n",
        "        # Se puede agregar más capas Densas en el medio si se desea\n",
        "\n",
        "        # Provide number of classes to the final layer:\n",
        "        output = tf.keras.layers.Dense(output_shape, activation='softmax')(output)\n",
        "\n",
        "        # Final model:\n",
        "        self.model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "        self.model.load_weights(self.model_weights_path)\n",
        "\n",
        "\n",
        "    def predict(self, input_text):\n",
        "\n",
        "        tf_batch = self.tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length, # truncates if len(s) > max_length\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            padding=\"max_length\", # pads to the right by default # CHECK THIS for pad_to_max_length\n",
        "            truncation=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "\n",
        "        X_ensayo = [tf_batch['input_ids'],  \n",
        "                    tf_batch['attention_mask']]\n",
        "\n",
        "        y_prob_ensayo = self.model.predict(X_ensayo)\n",
        "        y_prob = np.argmax(y_prob_ensayo, axis=1)\n",
        "        class_predicted = self.class_names[int(y_prob)]\n",
        "        print(\"Input:\", input_text)\n",
        "        print(\"Clasificacion:\", class_predicted)\n",
        "        return class_predicted\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# CREAR NUESTRA API con Flask\n",
        "# ------------------------------------------------------------------\n",
        "import traceback\n",
        "from flask import Flask, jsonify\n",
        "# Crear el server Flask\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Crear el modelo que utilizaremos en la API\n",
        "modelo = MyBertModel('bert_weights.h5')\n",
        "\n",
        "@app.route(\"/\")\n",
        "def index():\n",
        "    msg = '''Para poder solicitar una prediccion debe acceder al endpoint:\n",
        "            /predict/<input_text>\n",
        "            Debe reemplazar <input_text> en el explorador por el texto\n",
        "            que desea ingresar al modelo\n",
        "            '''\n",
        "    return msg\n",
        "\n",
        "@app.route(\"/predict/<input_text>\")\n",
        "def predict(input_text):\n",
        "    # Siempre es recomendable colocar nuestro\n",
        "    # código entre try except para que el servidor\n",
        "    # no se caiga si llega a fallar algo\n",
        "    try:\n",
        "        return modelo.predict(input_text)\n",
        "    except:\n",
        "        # En caso de falla, retornar el mensaje de error\n",
        "        return jsonify({'trace': traceback.format_exc()})\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Lanzar server\n",
        "    app.run(host=\"127.0.0.1\", port=5000, debug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd3IUVIv3VM_"
      },
      "source": [
        "# Exponer el puerto 5000 de colab al exterior\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks_L-nhK4bdf"
      },
      "source": [
        "# Lanzar la aplicacion\n",
        "!python app.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}