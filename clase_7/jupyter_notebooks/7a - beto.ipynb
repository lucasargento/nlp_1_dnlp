{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G63Jpt-wYcJ3"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Beto\n",
        "[GitHub LINK](https://github.com/dccuchile/beto)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcPiEBdt8NqM",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvoZ8YlK0vOQ"
      },
      "source": [
        "## 1 - BETO for Masked predictions\n",
        "Se necesita instalar la librería de \"transformers\" de Hugging Face para utilizar los modelos de BERT y sus funciones de ayuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vQGeFm0pdn1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJHhebCnnvnn",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import TFBertForMaskedLM, BertTokenizer\n",
        "\n",
        "# Muy importante que para tensorflow los modelos Bert deben empezar con \"TF\"\n",
        "# de lo contrario estaremos utilizando un modelo para pytorch\n",
        "\n",
        "# Descargamos el modelo base de BETO y su correspondiente tokenizer (BERT para español)\n",
        "model = TFBertForMaskedLM.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kmg5ZuNb1Bh6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1fBV3ElzDZf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "text = \"[CLS] Para solucionar los [MASK] del país, el presidente debe [MASK] de inmediato. [SEP]\"\n",
        "masked_indxs = (4,11)\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "# con `convert_tokens_to_ids` podemos mapear cada término al token correspondiente\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B2alArhzKVb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Sentencias transformadas en tokens\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwaq1y8gzMTe",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Tokens transformados a Ids\n",
        "print(indexed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCggZIN8c_OH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tokens_tensor = tokenizer(text, return_tensors='tf')\n",
        "tokens_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Nótese el tipo de tokens_tensor\n",
        "type(tokens_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt0T451xIdfI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "predictions = model(tokens_tensor)\n",
        "print(type(predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADwOxLevzyaS",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(\"Texto de entrada:\", text)\n",
        "\n",
        "predictions = model(tokens_tensor)[0]\n",
        "for i,midx in enumerate(masked_indxs):\n",
        "    # El sistema predice los posibles ids de palabras por cada MASK (en este caso 2)\n",
        "    # Se toman los 5 IDs con mayor probabilidad    \n",
        "    idxs = tf.argsort(predictions[0,midx], direction='DESCENDING')\n",
        "    # Esos IDs se transformar en palabras/tokens usando el tokenizar\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens(idxs[:5])\n",
        "    print('MASK',i,':',predicted_token)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "7a - beto.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
