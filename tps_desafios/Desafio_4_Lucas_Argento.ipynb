{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfa39F4lsLf3"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "## **Procesamiento de lenguaje natural**\n",
        "# **Desafio 4: Modelo de lenguaje - QA BOT**\n",
        "\n",
        "Se buscará construir un bot de preguntas y respuestas adaptando el ejemplo trabajado en clase del modelo traductor seq-to-seq basado en LSTMs.\n",
        "\n",
        "Utilizaremos el dataset de preguntas y respuestas de **SQAC (Spanish Question-Answering Corpus)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqO0PRcFsPTe"
      },
      "source": [
        "### **0. Datos e imports**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown --quiet"
      ],
      "metadata": {
        "id": "FGmtK8J19PNk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq3YXak9sGHd"
      },
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgYatMIdk_eT",
        "outputId": "c114f01c-89cf-4336-9fce-b89dbc45e02b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torchinfo\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYpIWGaXxfKe",
        "outputId": "75660a7e-b37f-4164-ff36-ff8dd31fd9f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.12/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import platform\n",
        "\n",
        "if os.access('torch_helpers.py', os.F_OK) is False:\n",
        "    if platform.system() == 'Windows':\n",
        "        !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py\n",
        "    else:\n",
        "        !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py"
      ],
      "metadata": {
        "id": "GHFPS5KNxgR9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### > Descarga de datos"
      ],
      "metadata": {
        "id": "5BFiCH8nxoIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se utilizaran datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en inglés. Se construirá un BOT para responder a preguntas del usuario (QA)."
      ],
      "metadata": {
        "id": "4oOJmvMlkWEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown --quiet"
      ],
      "metadata": {
        "id": "hEhdjpGQkiE4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "if os.access('data_volunteers.json', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n",
        "    output = 'data_volunteers.json'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"El dataset ya se encuentra descargado\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssB0HnggkkiK",
        "outputId": "66ad352a-386f-4076-eb4d-6f50506066f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El dataset ya se encuentra descargado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "text_file = \"data_volunteers.json\"\n",
        "with open(text_file) as f:\n",
        "    data = json.load(f) # la variable data será un diccionario"
      ],
      "metadata": {
        "id": "gjoWRrIqkkf0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Observar los campos disponibles en cada linea del dataset\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAfmXkH_kkUY",
        "outputId": "00074dc7-e47c-48b7-c680-f2e664df6302"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_in = []\n",
        "chat_out = []\n",
        "\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "max_len = 30\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt.replace(\"\\'d\", \" had\")\n",
        "    txt.replace(\"\\'s\", \" is\")\n",
        "    txt.replace(\"\\'m\", \" am\")\n",
        "    txt.replace(\"don't\", \"do not\")\n",
        "    txt = re.sub(r'\\W+', ' ', txt)\n",
        "\n",
        "    return txt\n",
        "\n",
        "for line in data:\n",
        "    for i in range(len(line['dialog'])-1):\n",
        "        # vamos separando el texto en \"preguntas\" (chat_in)\n",
        "        # y \"respuestas\" (chat_out)\n",
        "        chat_in = clean_text(line['dialog'][i]['text'])\n",
        "        chat_out = clean_text(line['dialog'][i+1]['text'])\n",
        "\n",
        "        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n",
        "            continue\n",
        "\n",
        "        input_sentence, output = chat_in, chat_out\n",
        "\n",
        "        # output sentence (decoder_output) tiene\n",
        "        output_sentence = output + \" \"\n",
        "        # output sentence input (decoder_input) tiene\n",
        "        output_sentence_input = \" \" + output\n",
        "\n",
        "        input_sentences.append(input_sentence)\n",
        "        output_sentences.append(output_sentence)\n",
        "        output_sentences_inputs.append(output_sentence_input)\n",
        "\n",
        "print(\"Cantidad de rows utilizadas:\", len(input_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rViiIeGtk0C6",
        "outputId": "9845ef57-ef19-411b-94da-9479fb30e87b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de rows utilizadas: 6033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences[1], output_sentences[1], output_sentences_inputs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrS4qxCUk2Hv",
        "outputId": "a00a76fa-fa80-4237-d9ee-31d306a3b456"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hi how are you ', 'not bad and you  ', ' not bad and you ')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P-ynUNP5xp6"
      },
      "source": [
        "### 2 - Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WAZGOTfGyha"
      },
      "source": [
        "# Definir el tamaño máximo del vocabulario\n",
        "MAX_VOCAB_SIZE = 8000"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF1W6peoFGXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f7b2d2-2d20-4c59-9144-806246ba3697"
      },
      "source": [
        "from torch_helpers import Tokenizer\n",
        "\n",
        "input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "input_tokenizer.fit_on_texts(input_sentences)\n",
        "\n",
        "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
        "\n",
        "word2idx_inputs = input_tokenizer.word_index\n",
        "print(\"Palabras en el vocabulario:\", len(word2idx_inputs))\n",
        "\n",
        "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
        "print(\"Sentencia de entrada más larga:\", max_input_len)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras en el vocabulario: 1799\n",
            "Sentencia de entrada más larga: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBzdKiTVIBYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ef09ab-9685-42e9-e5b4-8db2027b97c4"
      },
      "source": [
        "# A los filtros de símbolos del Tokenizer agregamos el \"¿\",\n",
        "# sacamos los \"<>\" para que no afectar nuestros tokens\n",
        "special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        "\n",
        "# Los agregás manualmente antes de entrenar el tokenizer\n",
        "output_tokenizer = Tokenizer(\n",
        "    num_words=MAX_VOCAB_SIZE,\n",
        "    filters='!\"#$%&()*+,-./:;=¿?@[\\\\]^_`{|}~\\t\\n'\n",
        ")\n",
        "\n",
        "output_tokenizer.fit_on_texts(special_tokens + output_sentences)\n",
        "\n",
        "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
        "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "word2idx_outputs = output_tokenizer.word_index\n",
        "idx2word_outputs = {i: w for w, i in word2idx_outputs.items()}\n",
        "\n",
        "print(\"Palabras en el vocabulario:\", len(word2idx_outputs))\n",
        "\n",
        "num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE) # Se suma 1 por el primer <sos>\n",
        "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
        "print(\"Sentencia de salida más larga:\", max_out_len)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras en el vocabulario: 1808\n",
            "Sentencia de salida más larga: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_TOKEN = output_tokenizer.word_index[\"<pad>\"]\n",
        "UNK_TOKEN = output_tokenizer.word_index[\"<unk>\"]\n",
        "SOS_TOKEN = output_tokenizer.word_index[\"<sos>\"]\n",
        "EOS_TOKEN = output_tokenizer.word_index[\"<eos>\"]"
      ],
      "metadata": {
        "id": "mHl5CDNz2zQX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgLC706EQx3p"
      },
      "source": [
        "# Por una cuestion de que no explote la RAM se limitará el tamaño de las sentencias de entrada\n",
        "# a la mitad:\n",
        "max_input_len = 18\n",
        "max_out_len = 18"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Vocab real: {len(word2idx_outputs)}\")\n",
        "print(f\"MAX_VOCAB_SIZE: {MAX_VOCAB_SIZE}\")\n",
        "print(f\"Embeddings realmente usados: {len(output_tokenizer.word_index)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogl5XMXtxFrq",
        "outputId": "ac5a6a8b-6777-45b8-8203-aaebb97400e8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab real: 1808\n",
            "MAX_VOCAB_SIZE: 8000\n",
            "Embeddings realmente usados: 1808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGOn9N57IuYz"
      },
      "source": [
        "> A la hora de realiza padding es importante teneer en cuenta que **en el encoder los ceros se agregan al comienzo** y **en el decoder al final**.\n",
        "\n",
        "> Esto es porque la salida del encoder está basado en las últimas palabras de la sentencia (son las más importantes), mientras que en el decoder está basado en el comienzo de la secuencia de salida ya que es la realimentación del sistema y termina con fin de sentencia."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ochiy5VP670t",
        "outputId": "25b71e43-d506-4e12-c096-32b38791d354"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1800"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Ob4hAWJkcv"
      },
      "source": [
        "import torch\n",
        "\n",
        "def pad_sequences_torch(sequences, maxlen, padding='pre', padding_value=PAD_TOKEN):\n",
        "    \"\"\"\n",
        "    Versión robusta: ignora secuencias vacías y evita RuntimeError.\n",
        "\n",
        "    Args:\n",
        "        sequences (list[list[int]]): lista de secuencias (ej. token ids)\n",
        "        maxlen (int): longitud máxima final de cada secuencia\n",
        "        padding ('pre' or 'post'): dónde agregar el padding\n",
        "        padding_value (int): valor de padding (por defecto 0)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: tensor de tamaño (num_seqs, maxlen)\n",
        "    \"\"\"\n",
        "    n = len(sequences)\n",
        "    padded = torch.full((n, maxlen), padding_value, dtype=torch.long)\n",
        "\n",
        "    for i, seq in enumerate(sequences):\n",
        "        # ⚠️ si la secuencia está vacía, saltar\n",
        "        if len(seq) == 0:\n",
        "            continue\n",
        "\n",
        "        seq = torch.tensor(seq[:maxlen], dtype=torch.long)\n",
        "        seq_len = len(seq)\n",
        "\n",
        "        if padding == 'pre':\n",
        "            padded[i, -seq_len:] = seq\n",
        "        else:  # 'post'\n",
        "            padded[i, :seq_len] = seq\n",
        "\n",
        "    return padded\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_sequences = pad_sequences_torch(input_integer_seq, maxlen=max_input_len, padding='pre')\n",
        "decoder_input_sequences = pad_sequences_torch(output_integer_seq, maxlen=max_out_len, padding='post')\n",
        "\n",
        "print(\"Encoder shape:\", encoder_input_sequences.shape)\n",
        "print(\"Decoder shape:\", decoder_input_sequences.shape)\n",
        "print(\"Encoder example:\\n\", encoder_input_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VySR1pzx9UG",
        "outputId": "f17bd62f-077c-4988-9d45-388a02f1a31b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder shape: torch.Size([6033, 18])\n",
            "Decoder shape: torch.Size([6033, 18])\n",
            "Encoder example:\n",
            " tensor([[1800, 1800, 1800,  ..., 1800, 1800,   19],\n",
            "        [1800, 1800, 1800,  ...,   10,    7,    2],\n",
            "        [1800, 1800, 1800,  ..., 1800, 1800,   11],\n",
            "        ...,\n",
            "        [1800, 1800, 1800,  ..., 1800, 1800,   19],\n",
            "        [1800, 1800, 1800,  ...,    3,    8,   13],\n",
            "        [1800, 1800, 1800,  ...,    2,  596,    3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_output_sequences = pad_sequences_torch(output_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"decoder_output_sequences shape:\", decoder_output_sequences.shape)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HLAwGVjm-zN",
        "outputId": "b9b7b3bf-da73-44c3-c5c1-9afa1e94f4de"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder_output_sequences shape: torch.Size([6033, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK4blEEsRQv3"
      },
      "source": [
        "> La última capa del modelo (softmax) necesita que los valores de salida\n",
        "del decoder (decoder_sequences) estén en formato oneHotEncoder.\n",
        "\n",
        "> Se utiliza \"decoder_output_sequences\" con la misma estrategia que se transformó la entrada del decoder."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QADataset(Dataset):\n",
        "    def __init__(self, encoder_input, decoder_input, decoder_output):\n",
        "        self.encoder_inputs = encoder_input.long()\n",
        "        self.decoder_inputs = decoder_input.long()\n",
        "        self.decoder_outputs = decoder_output.long()  # sin one-hot para CrossEntropyLoss\n",
        "\n",
        "        self.len = len(self.encoder_inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.encoder_inputs[idx],\n",
        "            self.decoder_inputs[idx],\n",
        "            self.decoder_outputs[idx]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "data_set = QADataset(encoder_input_sequences, decoder_input_sequences, decoder_output_sequences)\n",
        "\n",
        "encoder_input_size = data_set.encoder_inputs.shape[1]\n",
        "print(\"encoder_input_size:\", encoder_input_size)\n",
        "\n",
        "decoder_input_size = data_set.decoder_inputs.shape[1]\n",
        "print(\"decoder_input_size:\", decoder_input_size)\n",
        "\n",
        "output_dim = data_set.decoder_outputs.shape\n",
        "print(\"Output dim\", output_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD0bpM32yWfB",
        "outputId": "61e49598-687f-404c-a1f1-f3dd4f1d23d8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_input_size: 18\n",
            "decoder_input_size: 18\n",
            "Output dim torch.Size([6033, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "valid_set_size = int(data_set.len * 0.2)\n",
        "train_set_size = data_set.len - valid_set_size\n",
        "\n",
        "train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n",
        "valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len))\n",
        "\n",
        "print(\"Tamaño del conjunto de entrenamiento:\", len(train_set))\n",
        "print(\"Tamaño del conjunto de validacion:\", len(valid_set))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUDPZeuAU1RI",
        "outputId": "d2399d3e-e71a-4004-c5b4-b44b7c666388"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del conjunto de entrenamiento: 4827\n",
            "Tamaño del conjunto de validacion: 1206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJrcTRZ5z16S",
        "outputId": "8d38bc8c-ed95-4572-f224-beff03c4fd20"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello ',\n",
              " 'hi how are you ',\n",
              " 'hi ',\n",
              " 'hi ',\n",
              " 'hi ',\n",
              " 'where are you working ',\n",
              " 'bro ',\n",
              " 'where are you from ',\n",
              " 'i am from russia and you ',\n",
              " 'i hate them most of the time ']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_sentences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AU9bnZZ0Gsl",
        "outputId": "31cd7a97-24b0-4393-a6af-45e8a8482f59"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi how are you  ',\n",
              " 'not bad and you  ',\n",
              " 'hello  ',\n",
              " 'hello  ',\n",
              " 'hello how are you today  ',\n",
              " 'bro  ',\n",
              " 'where are you from  ',\n",
              " 'i am from russia and you  ',\n",
              " 'i am from the united states  ',\n",
              " 'you are racist  ']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CJIsLBbj6rg"
      },
      "source": [
        "### **3 - Preparar los embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OcT-DLzkHS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c2e02e-72c5-43a2-a0e5-28077a9246c9"
      },
      "source": [
        "import os\n",
        "import gdown\n",
        "if os.access('gloveembedding.pkl', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download'\n",
        "    output = 'gloveembedding.pkl'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"Los embeddings gloveembedding.pkl ya están descargados\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Los embeddings gloveembedding.pkl ya están descargados\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgqtV8GpkSc8"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "from io import StringIO\n",
        "import pickle\n",
        "\n",
        "class WordsEmbeddings(object):\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    def __init__(self):\n",
        "        # load the embeddings\n",
        "        words_embedding_pkl = Path(self.PKL_PATH)\n",
        "        if not words_embedding_pkl.is_file():\n",
        "            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n",
        "            assert words_embedding_txt.is_file(), 'Words embedding not available'\n",
        "            embeddings = self.convert_model_to_pickle()\n",
        "        else:\n",
        "            embeddings = self.load_model_from_pickle()\n",
        "        self.embeddings = embeddings\n",
        "        # build the vocabulary hashmap\n",
        "        index = np.arange(self.embeddings.shape[0])\n",
        "        # Dicctionarios para traducir de embedding a IDX de la palabra\n",
        "        self.word2idx = dict(zip(self.embeddings['word'], index))\n",
        "        self.idx2word = dict(zip(index, self.embeddings['word']))\n",
        "\n",
        "    def get_words_embeddings(self, words):\n",
        "        words_idxs = self.words2idxs(words)\n",
        "        return self.embeddings[words_idxs]['embedding']\n",
        "\n",
        "    def words2idxs(self, words):\n",
        "        return np.array([self.word2idx.get(word, -1) for word in words])\n",
        "\n",
        "    def idxs2words(self, idxs):\n",
        "        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n",
        "\n",
        "    def load_model_from_pickle(self):\n",
        "        self.logger.debug(\n",
        "            'loading words embeddings from pickle {}'.format(\n",
        "                self.PKL_PATH\n",
        "            )\n",
        "        )\n",
        "        max_bytes = 2**28 - 1 # 256MB\n",
        "        bytes_in = bytearray(0)\n",
        "        input_size = os.path.getsize(self.PKL_PATH)\n",
        "        with open(self.PKL_PATH, 'rb') as f_in:\n",
        "            for _ in range(0, input_size, max_bytes):\n",
        "                bytes_in += f_in.read(max_bytes)\n",
        "        embeddings = pickle.loads(bytes_in)\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "    def convert_model_to_pickle(self):\n",
        "        # create a numpy strctured array:\n",
        "        # word     embedding\n",
        "        # U50      np.float32[]\n",
        "        # word_1   a, b, c\n",
        "        # word_2   d, e, f\n",
        "        # ...\n",
        "        # word_n   g, h, i\n",
        "        self.logger.debug(\n",
        "            'converting and loading words embeddings from text file {}'.format(\n",
        "                self.WORD_TO_VEC_MODEL_TXT_PATH\n",
        "            )\n",
        "        )\n",
        "        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n",
        "                     ('embedding', np.float32, (self.N_FEATURES,))]\n",
        "        structure = np.dtype(structure)\n",
        "        # load numpy array from disk using a generator\n",
        "        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n",
        "            embeddings_gen = (\n",
        "                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n",
        "                if len(line.split()[1:]) == self.N_FEATURES\n",
        "            )\n",
        "            embeddings = np.fromiter(embeddings_gen, structure)\n",
        "        # add a null embedding\n",
        "        null_embedding = np.array(\n",
        "            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n",
        "            dtype=structure\n",
        "        )\n",
        "        embeddings = np.concatenate([embeddings, null_embedding])\n",
        "        # dump numpy array to disk using pickle\n",
        "        max_bytes = 2**28 - 1 # # 256MB\n",
        "        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(self.PKL_PATH, 'wb') as f_out:\n",
        "            for idx in range(0, len(bytes_out), max_bytes):\n",
        "                f_out.write(bytes_out[idx:idx+max_bytes])\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class GloveEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n",
        "    PKL_PATH = 'gloveembedding.pkl'\n",
        "    N_FEATURES = 50\n",
        "    WORD_MAX_SIZE = 60\n",
        "\n",
        "class FasttextEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n",
        "    PKL_PATH = 'fasttext.pkl'\n",
        "    N_FEATURES = 300\n",
        "    WORD_MAX_SIZE = 60"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mosj2-x-kXBK"
      },
      "source": [
        "model_embeddings = GloveEmbeddings()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9FS8ca1ke_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd1d48c-1be3-4f1a-c492-0d3acf376748"
      },
      "source": [
        "print(\"preparing embedding matrix...\")\n",
        "embed_dim = model_embeddings.N_FEATURES\n",
        "words_not_found = []\n",
        "\n",
        "special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        "for t in special_tokens:\n",
        "    if t not in word2idx_inputs:\n",
        "        word2idx_inputs[t] = len(word2idx_inputs) + 1\n",
        "\n",
        "nb_words = len(word2idx_inputs)\n",
        "embedding_matrix = np.zeros((nb_words + 1, embed_dim))\n",
        "\n",
        "for word, i in word2idx_inputs.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "\n",
        "    emb_vec = model_embeddings.get_words_embeddings([word])\n",
        "    if emb_vec is not None and len(emb_vec) > 0:\n",
        "        embedding_matrix[i] = emb_vec[0]\n",
        "    else:\n",
        "        words_not_found.append(word)\n",
        "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
        "\n",
        "# asignar tokens especiales explícitamente\n",
        "embedding_matrix[word2idx_inputs[\"<pad>\"]] = np.zeros((embed_dim,))\n",
        "for tok in [\"<unk>\", \"<sos>\", \"<eos>\"]:\n",
        "    embedding_matrix[word2idx_inputs[tok]] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
        "\n",
        "print(f\"number of null word embeddings: {np.sum(np.sum(embedding_matrix, axis=1) == 0)}\")\n",
        "print(f\"coverage: {(1 - np.sum(np.sum(embedding_matrix, axis=1) == 0)/embedding_matrix.shape[0])*100:.2f}%\")\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preparing embedding matrix...\n",
            "number of null word embeddings: 140\n",
            "coverage: 92.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vKbhjtIwPgM"
      },
      "source": [
        "### **4 - Entrenar el modelo**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_acc(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Calcula accuracy secuencia a secuencia.\n",
        "    \"\"\"\n",
        "    y_pred_tags = y_pred.argmax(dim=-1)\n",
        "    correct = (y_pred_tags == y_true).float()\n",
        "    acc = correct.sum() / correct.numel()\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train(model, train_loader, valid_loader, optimizer, criterion, epochs=30):\n",
        "    train_loss, train_accuracy = [], []\n",
        "    valid_loss, valid_accuracy = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_train_loss, epoch_train_acc = 0.0, 0.0\n",
        "\n",
        "        for train_encoder_input, train_decoder_input, train_target in train_loader:\n",
        "            # --- mover a GPU ---\n",
        "            train_encoder_input = train_encoder_input.to(device)\n",
        "            train_decoder_input = train_decoder_input.to(device)\n",
        "            train_target = train_target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # --- forward ---\n",
        "            output = model(train_encoder_input, train_decoder_input)  # (batch, seq_len, vocab)\n",
        "\n",
        "            # --- calcular loss (vectorizado) ---\n",
        "            loss = criterion(\n",
        "                output.view(-1, output.shape[-1]),   # (batch*seq, vocab)\n",
        "                train_target.view(-1)                # (batch*seq)\n",
        "            )\n",
        "\n",
        "            # --- backward ---\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # --- metrics ---\n",
        "            epoch_train_loss += loss.item()\n",
        "            epoch_train_acc += sequence_acc(output, train_target).item()\n",
        "\n",
        "        # promedio de epoch\n",
        "        train_loss.append(epoch_train_loss / len(train_loader))\n",
        "        train_accuracy.append(epoch_train_acc / len(train_loader))\n",
        "\n",
        "        # =====================================================\n",
        "        # 🔹 VALIDACIÓN\n",
        "        # =====================================================\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_encoder_input, val_decoder_input, val_target = next(iter(valid_loader))\n",
        "            val_encoder_input = val_encoder_input.to(device)\n",
        "            val_decoder_input = val_decoder_input.to(device)\n",
        "            val_target = val_target.to(device)\n",
        "\n",
        "            val_output = model(val_encoder_input, val_decoder_input)\n",
        "\n",
        "            val_loss = criterion(\n",
        "                val_output.view(-1, val_output.shape[-1]),\n",
        "                val_target.view(-1)\n",
        "            ).item()\n",
        "\n",
        "            val_acc = sequence_acc(val_output, val_target).item()\n",
        "\n",
        "        valid_loss.append(val_loss)\n",
        "        valid_accuracy.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:02}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss[-1]:.3f} | Train Acc: {train_accuracy[-1]:.3f} | \"\n",
        "              f\"Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.3f}\")\n",
        "\n",
        "    return {\n",
        "        \"loss\": train_loss,\n",
        "        \"accuracy\": train_accuracy,\n",
        "        \"val_loss\": valid_loss,\n",
        "        \"val_accuracy\": valid_accuracy,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "khMYFwFqolTk"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "PAD_TOKEN = word2idx_inputs.get(\"<pad>\", 0)\n",
        "VOCAB_SIZE = embedding_matrix.shape[0]\n",
        "EMBED_DIM = embedding_matrix.shape[1]\n",
        "\n",
        "print(f\"✅ Using device: {device}\")\n",
        "print(f\"Vocab size: {VOCAB_SIZE}, Embedding dim: {EMBED_DIM}\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Definición de modelos\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_matrix, pad_token, hidden_size=128, num_layers=1, train_embeddings=True):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_matrix.shape[1], padding_idx=pad_token)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = train_embeddings  # desfreezado\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_matrix.shape[1],\n",
        "            hidden_size=hidden_size,\n",
        "            batch_first=True,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        outputs, (h, c) = self.lstm(emb)\n",
        "        return (h, c)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_matrix, pad_token, hidden_size=128, num_layers=1, train_embeddings=True):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_matrix.shape[1], padding_idx=pad_token)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = train_embeddings  # desfreezado\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_matrix.shape[1],\n",
        "            hidden_size=hidden_size,\n",
        "            batch_first=True,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        emb = self.embedding(x)\n",
        "        output, (h, c) = self.lstm(emb, prev_state)\n",
        "        logits = self.fc(output[:, -1, :])\n",
        "        return logits, (h, c)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, encoder_input, decoder_input):\n",
        "        batch_size, seq_len = decoder_input.shape\n",
        "        vocab_size = self.decoder.fc.out_features\n",
        "        outputs = torch.zeros(batch_size, seq_len, vocab_size).to(encoder_input.device)\n",
        "        prev_state = self.encoder(encoder_input)\n",
        "\n",
        "        input_tok = decoder_input[:, 0:1]\n",
        "        for t in range(seq_len):\n",
        "            output, prev_state = self.decoder(input_tok, prev_state)\n",
        "            outputs[:, t, :] = output\n",
        "            input_tok = decoder_input[:, t:t+1]  # teacher forcing\n",
        "        return outputs\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Instanciación\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "encoder = Encoder(VOCAB_SIZE, embedding_matrix, PAD_TOKEN)\n",
        "decoder = Decoder(VOCAB_SIZE, embedding_matrix, PAD_TOKEN)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"✅ Modelo, optimizer y criterion inicializados correctamente\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K82gEEpJ4nUt",
        "outputId": "a547a48c-0508-43dc-994a-8616736397b0"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using device: cuda\n",
            "Vocab size: 1804, Embedding dim: 50\n",
            "✅ Modelo, optimizer y criterion inicializados correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_matrix.shape[0], \"==\", len(word2idx_inputs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prng84mr6lW4",
        "outputId": "25466377-02f4-415e-fba8-1d7940a365ef"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1804 == 1803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_TOKEN = word2idx_inputs.get(\"<pad>\", 0)\n",
        "PAD_TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTgrUs9v6qc_",
        "outputId": "0c37912c-f626-4c74-ffa5-afb06ca67b8a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1800"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2nprd6Pp1TG",
        "outputId": "b5208a8f-b159-4bdc-e542-5805874bde41"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(1804, 50, padding_idx=1800)\n",
              "    (lstm): LSTM(50, 128, batch_first=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(1804, 50, padding_idx=1800)\n",
              "    (lstm): LSTM(50, 128, batch_first=True)\n",
              "    (fc): Linear(in_features=128, out_features=1804, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history1 = train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    valid_loader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    epochs=50\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDB0KWIegt8s",
        "outputId": "42105ba8-c098-46a1-c0ac-edc07bb064b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/50 | Train Loss: 2.858 | Train Acc: 0.114 | Val Loss: 3.260 | Val Acc: 0.087\n",
            "Epoch 02/50 | Train Loss: 2.644 | Train Acc: 0.120 | Val Loss: 3.078 | Val Acc: 0.095\n",
            "Epoch 03/50 | Train Loss: 2.469 | Train Acc: 0.126 | Val Loss: 2.944 | Val Acc: 0.104\n",
            "Epoch 04/50 | Train Loss: 2.320 | Train Acc: 0.131 | Val Loss: 2.877 | Val Acc: 0.109\n",
            "Epoch 05/50 | Train Loss: 2.184 | Train Acc: 0.134 | Val Loss: 2.816 | Val Acc: 0.113\n",
            "Epoch 06/50 | Train Loss: 2.069 | Train Acc: 0.136 | Val Loss: 2.730 | Val Acc: 0.113\n",
            "Epoch 07/50 | Train Loss: 1.962 | Train Acc: 0.138 | Val Loss: 2.701 | Val Acc: 0.122\n",
            "Epoch 08/50 | Train Loss: 1.869 | Train Acc: 0.141 | Val Loss: 2.634 | Val Acc: 0.122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = range(1, len(history1['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history1['accuracy'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=history1['val_accuracy'], label='valid')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pZzm3tx059Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Modelo con embeddings freezados"
      ],
      "metadata": {
        "id": "swlBRvLJ-U9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(VOCAB_SIZE, embedding_matrix, PAD_TOKEN, train_embeddings=False)\n",
        "decoder = Decoder(VOCAB_SIZE, embedding_matrix, PAD_TOKEN,  train_embeddings=False)\n",
        "model_freezed = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"✅ Modelo, optimizer y criterion inicializados correctamente\")\n",
        "\n",
        "history2 = train(\n",
        "    model_freezed,\n",
        "    train_loader,\n",
        "    valid_loader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "epoch_count = range(1, len(history2['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history2['accuracy'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=history2['val_accuracy'], label='valid')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9j2rqeYo-UcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbwn0ekDy_s2"
      },
      "source": [
        "### **5 - Inferencia**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "def infer(model, input_seq, word2idx_output, idx2word_output,\n",
        "          max_output_len=20, device=\"cuda\",\n",
        "          beam_size=3, temperature=1.0, stochastic=False):\n",
        "    \"\"\"\n",
        "    Inferencia con opción de greedy o beam search.\n",
        "\n",
        "    Args:\n",
        "        model: modelo Seq2Seq (encoder + decoder)\n",
        "        input_seq: secuencia tokenizada de entrada (lista o tensor)\n",
        "        word2idx_output: vocab del decoder (str → int)\n",
        "        idx2word_output: vocab inverso (int → str)\n",
        "        max_output_len: largo máximo de la secuencia generada\n",
        "        beam_size: cantidad de beams (1 = greedy)\n",
        "        temperature: suaviza distribución en modo estocástico\n",
        "        stochastic: si True, elige tokens por muestreo en vez de argmax\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # --- 1️⃣ Codificación ---\n",
        "        input_tensor = torch.tensor([input_seq], dtype=torch.long).to(device)\n",
        "        encoder_hidden, encoder_cell = model.encoder(input_tensor)\n",
        "\n",
        "        sos_token = word2idx_output.get(\"<sos>\", 1)\n",
        "        eos_token = word2idx_output.get(\"<eos>\", 2)\n",
        "\n",
        "        # --- 2️⃣ Inicialización de beams ---\n",
        "        beams = [{\n",
        "            \"tokens\": [sos_token],\n",
        "            \"hidden\": encoder_hidden,\n",
        "            \"cell\": encoder_cell,\n",
        "            \"logprob\": 0.0\n",
        "        }]\n",
        "\n",
        "        # --- 3️⃣ Decodificación autoregresiva ---\n",
        "        for _ in range(max_output_len):\n",
        "            new_beams = []\n",
        "            for beam in beams:\n",
        "                last_token = beam[\"tokens\"][-1]\n",
        "                if last_token == eos_token:\n",
        "                    new_beams.append(beam)\n",
        "                    continue\n",
        "\n",
        "                decoder_input = torch.tensor([[last_token]], dtype=torch.long).to(device)\n",
        "                output, (h, c) = model.decoder(decoder_input, (beam[\"hidden\"], beam[\"cell\"]))\n",
        "\n",
        "                # logits → probabilidades\n",
        "                probs = torch.softmax(output.squeeze(0) / temperature, dim=-1).cpu().numpy()\n",
        "\n",
        "                if stochastic:\n",
        "                    # muestreo aleatorio ponderado por softmax\n",
        "                    top_indices = np.random.choice(len(probs), beam_size, p=probs / probs.sum())\n",
        "                else:\n",
        "                    # top-k determinístico\n",
        "                    top_indices = probs.argsort()[-beam_size:][::-1]\n",
        "\n",
        "                for idx in top_indices:\n",
        "                    new_beams.append({\n",
        "                        \"tokens\": beam[\"tokens\"] + [idx],\n",
        "                        \"hidden\": h,\n",
        "                        \"cell\": c,\n",
        "                        \"logprob\": beam[\"logprob\"] + np.log(probs[idx] + 1e-10)\n",
        "                    })\n",
        "\n",
        "            # mantener los mejores beams según logprob promedio\n",
        "            beams = sorted(new_beams, key=lambda b: b[\"logprob\"] / len(b[\"tokens\"]), reverse=True)[:beam_size]\n",
        "\n",
        "        # --- 4️⃣ Seleccionar mejor secuencia ---\n",
        "        best_beam = max(beams, key=lambda b: b[\"logprob\"] / len(b[\"tokens\"]))\n",
        "        decoded_tokens = [\n",
        "            tok for tok in best_beam[\"tokens\"]\n",
        "            if tok not in [sos_token, eos_token]\n",
        "        ]\n",
        "\n",
        "        # --- 5️⃣ Decodificar texto ---\n",
        "        decoded_sentence = \" \".join([idx2word_output.get(tok, \"<unk>\") for tok in decoded_tokens])\n",
        "\n",
        "    return decoded_sentence, [b[\"tokens\"] for b in beams]  # devuelve también todos los beams\n"
      ],
      "metadata": {
        "id": "P80VW4vTrWMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentence(sentence, tokenizer, max_len):\n",
        "    # limpiar y convertir a índices usando el tokenizer\n",
        "    seq = tokenizer.texts_to_sequences([sentence.lower().strip()])[0]\n",
        "\n",
        "    # aplicar padding pre para que tenga longitud fija igual a la del entrenamiento\n",
        "    if len(seq) < max_len:\n",
        "        seq = [0] * (max_len - len(seq)) + seq\n",
        "    else:\n",
        "        seq = seq[-max_len:]\n",
        "\n",
        "    return seq\n"
      ],
      "metadata": {
        "id": "xefNxladr6Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_sequences[10]"
      ],
      "metadata": {
        "id": "JXv-bLgh0Pel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences[10]"
      ],
      "metadata": {
        "id": "K6XCDxHO0g_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> #### Modelo con embeddings entrenables"
      ],
      "metadata": {
        "id": "IxbCkX6q-wTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"i love disney movies \"\n",
        "input_seq = encode_sentence(question, input_tokenizer, max_input_len)\n",
        "\n",
        "generated_answer, beams = infer(\n",
        "    model,\n",
        "    input_seq=input_seq,\n",
        "    word2idx_output=word2idx_outputs,\n",
        "    idx2word_output=idx2word_outputs,\n",
        "    beam_size=5,\n",
        "    temperature=0.8,\n",
        "    stochastic=True,   # False para beam search determinístico\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"Q: {question}\")\n",
        "print(f\"A: {generated_answer}\")\n"
      ],
      "metadata": {
        "id": "e4_IJi-jrWJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> #### Modelo con embeddings freezados"
      ],
      "metadata": {
        "id": "QHZ5WOcx-yry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"i love disney movies \"\n",
        "input_seq = encode_sentence(question, input_tokenizer, max_input_len)\n",
        "\n",
        "generated_answer, beams = infer(\n",
        "    model_freezed,\n",
        "    input_seq=input_seq,\n",
        "    word2idx_output=word2idx_outputs,\n",
        "    idx2word_output=idx2word_outputs,\n",
        "    beam_size=5,\n",
        "    temperature=0.8,\n",
        "    stochastic=True,   # False para beam search determinístico\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"Q: {question}\")\n",
        "print(f\"A: {generated_answer}\")\n"
      ],
      "metadata": {
        "id": "LKU7zAEm-3IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkOjSJweqdF8"
      },
      "source": [
        "### 6 - Conclusión\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rualuy2NrXDW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}