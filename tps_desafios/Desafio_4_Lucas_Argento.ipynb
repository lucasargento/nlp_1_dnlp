{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfa39F4lsLf3"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "## **Procesamiento de lenguaje natural**\n",
        "# **Desafio 4: Modelo de lenguaje - QA BOT**\n",
        "\n",
        "Se buscar√° construir un bot de preguntas y respuestas adaptando el ejemplo trabajado en clase del modelo traductor seq-to-seq basado en LSTMs.\n",
        "\n",
        "Utilizaremos el dataset de preguntas y respuestas de **SQAC (Spanish Question-Answering Corpus)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqO0PRcFsPTe"
      },
      "source": [
        "### **0. Datos e imports**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown --quiet"
      ],
      "metadata": {
        "id": "FGmtK8J19PNk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq3YXak9sGHd"
      },
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgYatMIdk_eT",
        "outputId": "c114f01c-89cf-4336-9fce-b89dbc45e02b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torchinfo\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYpIWGaXxfKe",
        "outputId": "75660a7e-b37f-4164-ff36-ff8dd31fd9f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.12/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import platform\n",
        "\n",
        "if os.access('torch_helpers.py', os.F_OK) is False:\n",
        "    if platform.system() == 'Windows':\n",
        "        !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py\n",
        "    else:\n",
        "        !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py"
      ],
      "metadata": {
        "id": "GHFPS5KNxgR9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### > Descarga de datos"
      ],
      "metadata": {
        "id": "5BFiCH8nxoIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se utilizaran datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en ingl√©s. Se construir√° un BOT para responder a preguntas del usuario (QA)."
      ],
      "metadata": {
        "id": "4oOJmvMlkWEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown --quiet"
      ],
      "metadata": {
        "id": "hEhdjpGQkiE4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "if os.access('data_volunteers.json', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n",
        "    output = 'data_volunteers.json'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"El dataset ya se encuentra descargado\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssB0HnggkkiK",
        "outputId": "66ad352a-386f-4076-eb4d-6f50506066f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El dataset ya se encuentra descargado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "text_file = \"data_volunteers.json\"\n",
        "with open(text_file) as f:\n",
        "    data = json.load(f) # la variable data ser√° un diccionario"
      ],
      "metadata": {
        "id": "gjoWRrIqkkf0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Observar los campos disponibles en cada linea del dataset\n",
        "data[0].keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAfmXkH_kkUY",
        "outputId": "00074dc7-e47c-48b7-c680-f2e664df6302"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_in = []\n",
        "chat_out = []\n",
        "\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "output_sentences_inputs = []\n",
        "max_len = 30\n",
        "\n",
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt.replace(\"\\'d\", \" had\")\n",
        "    txt.replace(\"\\'s\", \" is\")\n",
        "    txt.replace(\"\\'m\", \" am\")\n",
        "    txt.replace(\"don't\", \"do not\")\n",
        "    txt = re.sub(r'\\W+', ' ', txt)\n",
        "\n",
        "    return txt\n",
        "\n",
        "for line in data:\n",
        "    for i in range(len(line['dialog'])-1):\n",
        "        # vamos separando el texto en \"preguntas\" (chat_in)\n",
        "        # y \"respuestas\" (chat_out)\n",
        "        chat_in = clean_text(line['dialog'][i]['text'])\n",
        "        chat_out = clean_text(line['dialog'][i+1]['text'])\n",
        "\n",
        "        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n",
        "            continue\n",
        "\n",
        "        input_sentence, output = chat_in, chat_out\n",
        "\n",
        "        # output sentence (decoder_output) tiene\n",
        "        output_sentence = output + \" \"\n",
        "        # output sentence input (decoder_input) tiene\n",
        "        output_sentence_input = \" \" + output\n",
        "\n",
        "        input_sentences.append(input_sentence)\n",
        "        output_sentences.append(output_sentence)\n",
        "        output_sentences_inputs.append(output_sentence_input)\n",
        "\n",
        "print(\"Cantidad de rows utilizadas:\", len(input_sentences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rViiIeGtk0C6",
        "outputId": "9845ef57-ef19-411b-94da-9479fb30e87b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de rows utilizadas: 6033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences[1], output_sentences[1], output_sentences_inputs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrS4qxCUk2Hv",
        "outputId": "a00a76fa-fa80-4237-d9ee-31d306a3b456"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hi how are you ', 'not bad and you  ', ' not bad and you ')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P-ynUNP5xp6"
      },
      "source": [
        "### 2 - Preprocesamiento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WAZGOTfGyha"
      },
      "source": [
        "# Definir el tama√±o m√°ximo del vocabulario\n",
        "MAX_VOCAB_SIZE = 8000"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF1W6peoFGXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f7b2d2-2d20-4c59-9144-806246ba3697"
      },
      "source": [
        "from torch_helpers import Tokenizer\n",
        "\n",
        "input_tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "input_tokenizer.fit_on_texts(input_sentences)\n",
        "\n",
        "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
        "\n",
        "word2idx_inputs = input_tokenizer.word_index\n",
        "print(\"Palabras en el vocabulario:\", len(word2idx_inputs))\n",
        "\n",
        "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
        "print(\"Sentencia de entrada m√°s larga:\", max_input_len)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras en el vocabulario: 1799\n",
            "Sentencia de entrada m√°s larga: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBzdKiTVIBYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ef09ab-9685-42e9-e5b4-8db2027b97c4"
      },
      "source": [
        "# A los filtros de s√≠mbolos del Tokenizer agregamos el \"¬ø\",\n",
        "# sacamos los \"<>\" para que no afectar nuestros tokens\n",
        "special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        "\n",
        "# Los agreg√°s manualmente antes de entrenar el tokenizer\n",
        "output_tokenizer = Tokenizer(\n",
        "    num_words=MAX_VOCAB_SIZE,\n",
        "    filters='!\"#$%&()*+,-./:;=¬ø?@[\\\\]^_`{|}~\\t\\n'\n",
        ")\n",
        "\n",
        "output_tokenizer.fit_on_texts(special_tokens + output_sentences)\n",
        "\n",
        "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
        "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
        "\n",
        "word2idx_outputs = output_tokenizer.word_index\n",
        "idx2word_outputs = {i: w for w, i in word2idx_outputs.items()}\n",
        "\n",
        "print(\"Palabras en el vocabulario:\", len(word2idx_outputs))\n",
        "\n",
        "num_words_output = min(len(word2idx_outputs) + 1, MAX_VOCAB_SIZE) # Se suma 1 por el primer <sos>\n",
        "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
        "print(\"Sentencia de salida m√°s larga:\", max_out_len)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras en el vocabulario: 1808\n",
            "Sentencia de salida m√°s larga: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_TOKEN = output_tokenizer.word_index[\"<pad>\"]\n",
        "UNK_TOKEN = output_tokenizer.word_index[\"<unk>\"]\n",
        "SOS_TOKEN = output_tokenizer.word_index[\"<sos>\"]\n",
        "EOS_TOKEN = output_tokenizer.word_index[\"<eos>\"]"
      ],
      "metadata": {
        "id": "mHl5CDNz2zQX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgLC706EQx3p"
      },
      "source": [
        "# Por una cuestion de que no explote la RAM se limitar√° el tama√±o de las sentencias de entrada\n",
        "# a la mitad:\n",
        "max_input_len = 18\n",
        "max_out_len = 18"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Vocab real: {len(word2idx_outputs)}\")\n",
        "print(f\"MAX_VOCAB_SIZE: {MAX_VOCAB_SIZE}\")\n",
        "print(f\"Embeddings realmente usados: {len(output_tokenizer.word_index)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogl5XMXtxFrq",
        "outputId": "ac5a6a8b-6777-45b8-8203-aaebb97400e8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab real: 1808\n",
            "MAX_VOCAB_SIZE: 8000\n",
            "Embeddings realmente usados: 1808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGOn9N57IuYz"
      },
      "source": [
        "> A la hora de realiza padding es importante teneer en cuenta que **en el encoder los ceros se agregan al comienzo** y **en el decoder al final**.\n",
        "\n",
        "> Esto es porque la salida del encoder est√° basado en las √∫ltimas palabras de la sentencia (son las m√°s importantes), mientras que en el decoder est√° basado en el comienzo de la secuencia de salida ya que es la realimentaci√≥n del sistema y termina con fin de sentencia."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ochiy5VP670t",
        "outputId": "25b71e43-d506-4e12-c096-32b38791d354"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1800"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0Ob4hAWJkcv"
      },
      "source": [
        "import torch\n",
        "\n",
        "def pad_sequences_torch(sequences, maxlen, padding='pre', padding_value=PAD_TOKEN):\n",
        "    \"\"\"\n",
        "    Versi√≥n robusta: ignora secuencias vac√≠as y evita RuntimeError.\n",
        "\n",
        "    Args:\n",
        "        sequences (list[list[int]]): lista de secuencias (ej. token ids)\n",
        "        maxlen (int): longitud m√°xima final de cada secuencia\n",
        "        padding ('pre' or 'post'): d√≥nde agregar el padding\n",
        "        padding_value (int): valor de padding (por defecto 0)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: tensor de tama√±o (num_seqs, maxlen)\n",
        "    \"\"\"\n",
        "    n = len(sequences)\n",
        "    padded = torch.full((n, maxlen), padding_value, dtype=torch.long)\n",
        "\n",
        "    for i, seq in enumerate(sequences):\n",
        "        # ‚ö†Ô∏è si la secuencia est√° vac√≠a, saltar\n",
        "        if len(seq) == 0:\n",
        "            continue\n",
        "\n",
        "        seq = torch.tensor(seq[:maxlen], dtype=torch.long)\n",
        "        seq_len = len(seq)\n",
        "\n",
        "        if padding == 'pre':\n",
        "            padded[i, -seq_len:] = seq\n",
        "        else:  # 'post'\n",
        "            padded[i, :seq_len] = seq\n",
        "\n",
        "    return padded\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_sequences = pad_sequences_torch(input_integer_seq, maxlen=max_input_len, padding='pre')\n",
        "decoder_input_sequences = pad_sequences_torch(output_integer_seq, maxlen=max_out_len, padding='post')\n",
        "\n",
        "print(\"Encoder shape:\", encoder_input_sequences.shape)\n",
        "print(\"Decoder shape:\", decoder_input_sequences.shape)\n",
        "print(\"Encoder example:\\n\", encoder_input_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VySR1pzx9UG",
        "outputId": "f17bd62f-077c-4988-9d45-388a02f1a31b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder shape: torch.Size([6033, 18])\n",
            "Decoder shape: torch.Size([6033, 18])\n",
            "Encoder example:\n",
            " tensor([[1800, 1800, 1800,  ..., 1800, 1800,   19],\n",
            "        [1800, 1800, 1800,  ...,   10,    7,    2],\n",
            "        [1800, 1800, 1800,  ..., 1800, 1800,   11],\n",
            "        ...,\n",
            "        [1800, 1800, 1800,  ..., 1800, 1800,   19],\n",
            "        [1800, 1800, 1800,  ...,    3,    8,   13],\n",
            "        [1800, 1800, 1800,  ...,    2,  596,    3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_output_sequences = pad_sequences_torch(output_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"decoder_output_sequences shape:\", decoder_output_sequences.shape)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HLAwGVjm-zN",
        "outputId": "b9b7b3bf-da73-44c3-c5c1-9afa1e94f4de"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "decoder_output_sequences shape: torch.Size([6033, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK4blEEsRQv3"
      },
      "source": [
        "> La √∫ltima capa del modelo (softmax) necesita que los valores de salida\n",
        "del decoder (decoder_sequences) est√©n en formato oneHotEncoder.\n",
        "\n",
        "> Se utiliza \"decoder_output_sequences\" con la misma estrategia que se transform√≥ la entrada del decoder."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QADataset(Dataset):\n",
        "    def __init__(self, encoder_input, decoder_input, decoder_output):\n",
        "        self.encoder_inputs = encoder_input.long()\n",
        "        self.decoder_inputs = decoder_input.long()\n",
        "        self.decoder_outputs = decoder_output.long()  # sin one-hot para CrossEntropyLoss\n",
        "\n",
        "        self.len = len(self.encoder_inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.encoder_inputs[idx],\n",
        "            self.decoder_inputs[idx],\n",
        "            self.decoder_outputs[idx]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "data_set = QADataset(encoder_input_sequences, decoder_input_sequences, decoder_output_sequences)\n",
        "\n",
        "encoder_input_size = data_set.encoder_inputs.shape[1]\n",
        "print(\"encoder_input_size:\", encoder_input_size)\n",
        "\n",
        "decoder_input_size = data_set.decoder_inputs.shape[1]\n",
        "print(\"decoder_input_size:\", decoder_input_size)\n",
        "\n",
        "output_dim = data_set.decoder_outputs.shape\n",
        "print(\"Output dim\", output_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SD0bpM32yWfB",
        "outputId": "61e49598-687f-404c-a1f1-f3dd4f1d23d8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoder_input_size: 18\n",
            "decoder_input_size: 18\n",
            "Output dim torch.Size([6033, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "valid_set_size = int(data_set.len * 0.2)\n",
        "train_set_size = data_set.len - valid_set_size\n",
        "\n",
        "train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n",
        "valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len))\n",
        "\n",
        "print(\"Tama√±o del conjunto de entrenamiento:\", len(train_set))\n",
        "print(\"Tama√±o del conjunto de validacion:\", len(valid_set))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUDPZeuAU1RI",
        "outputId": "d2399d3e-e71a-4004-c5b4-b44b7c666388"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tama√±o del conjunto de entrenamiento: 4827\n",
            "Tama√±o del conjunto de validacion: 1206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJrcTRZ5z16S",
        "outputId": "8d38bc8c-ed95-4572-f224-beff03c4fd20"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello ',\n",
              " 'hi how are you ',\n",
              " 'hi ',\n",
              " 'hi ',\n",
              " 'hi ',\n",
              " 'where are you working ',\n",
              " 'bro ',\n",
              " 'where are you from ',\n",
              " 'i am from russia and you ',\n",
              " 'i hate them most of the time ']"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_sentences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AU9bnZZ0Gsl",
        "outputId": "31cd7a97-24b0-4393-a6af-45e8a8482f59"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi how are you  ',\n",
              " 'not bad and you  ',\n",
              " 'hello  ',\n",
              " 'hello  ',\n",
              " 'hello how are you today  ',\n",
              " 'bro  ',\n",
              " 'where are you from  ',\n",
              " 'i am from russia and you  ',\n",
              " 'i am from the united states  ',\n",
              " 'you are racist  ']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CJIsLBbj6rg"
      },
      "source": [
        "### **3 - Preparar los embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OcT-DLzkHS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c2e02e-72c5-43a2-a0e5-28077a9246c9"
      },
      "source": [
        "import os\n",
        "import gdown\n",
        "if os.access('gloveembedding.pkl', os.F_OK) is False:\n",
        "    url = 'https://drive.google.com/uc?id=1wlDBOrxPq2-3htQ6ryVo7K1XnzLcfh4r&export=download'\n",
        "    output = 'gloveembedding.pkl'\n",
        "    gdown.download(url, output, quiet=False)\n",
        "else:\n",
        "    print(\"Los embeddings gloveembedding.pkl ya est√°n descargados\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Los embeddings gloveembedding.pkl ya est√°n descargados\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgqtV8GpkSc8"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "from io import StringIO\n",
        "import pickle\n",
        "\n",
        "class WordsEmbeddings(object):\n",
        "    logger = logging.getLogger(__name__)\n",
        "\n",
        "    def __init__(self):\n",
        "        # load the embeddings\n",
        "        words_embedding_pkl = Path(self.PKL_PATH)\n",
        "        if not words_embedding_pkl.is_file():\n",
        "            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n",
        "            assert words_embedding_txt.is_file(), 'Words embedding not available'\n",
        "            embeddings = self.convert_model_to_pickle()\n",
        "        else:\n",
        "            embeddings = self.load_model_from_pickle()\n",
        "        self.embeddings = embeddings\n",
        "        # build the vocabulary hashmap\n",
        "        index = np.arange(self.embeddings.shape[0])\n",
        "        # Dicctionarios para traducir de embedding a IDX de la palabra\n",
        "        self.word2idx = dict(zip(self.embeddings['word'], index))\n",
        "        self.idx2word = dict(zip(index, self.embeddings['word']))\n",
        "\n",
        "    def get_words_embeddings(self, words):\n",
        "        words_idxs = self.words2idxs(words)\n",
        "        return self.embeddings[words_idxs]['embedding']\n",
        "\n",
        "    def words2idxs(self, words):\n",
        "        return np.array([self.word2idx.get(word, -1) for word in words])\n",
        "\n",
        "    def idxs2words(self, idxs):\n",
        "        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n",
        "\n",
        "    def load_model_from_pickle(self):\n",
        "        self.logger.debug(\n",
        "            'loading words embeddings from pickle {}'.format(\n",
        "                self.PKL_PATH\n",
        "            )\n",
        "        )\n",
        "        max_bytes = 2**28 - 1 # 256MB\n",
        "        bytes_in = bytearray(0)\n",
        "        input_size = os.path.getsize(self.PKL_PATH)\n",
        "        with open(self.PKL_PATH, 'rb') as f_in:\n",
        "            for _ in range(0, input_size, max_bytes):\n",
        "                bytes_in += f_in.read(max_bytes)\n",
        "        embeddings = pickle.loads(bytes_in)\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "    def convert_model_to_pickle(self):\n",
        "        # create a numpy strctured array:\n",
        "        # word     embedding\n",
        "        # U50      np.float32[]\n",
        "        # word_1   a, b, c\n",
        "        # word_2   d, e, f\n",
        "        # ...\n",
        "        # word_n   g, h, i\n",
        "        self.logger.debug(\n",
        "            'converting and loading words embeddings from text file {}'.format(\n",
        "                self.WORD_TO_VEC_MODEL_TXT_PATH\n",
        "            )\n",
        "        )\n",
        "        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n",
        "                     ('embedding', np.float32, (self.N_FEATURES,))]\n",
        "        structure = np.dtype(structure)\n",
        "        # load numpy array from disk using a generator\n",
        "        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n",
        "            embeddings_gen = (\n",
        "                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n",
        "                if len(line.split()[1:]) == self.N_FEATURES\n",
        "            )\n",
        "            embeddings = np.fromiter(embeddings_gen, structure)\n",
        "        # add a null embedding\n",
        "        null_embedding = np.array(\n",
        "            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n",
        "            dtype=structure\n",
        "        )\n",
        "        embeddings = np.concatenate([embeddings, null_embedding])\n",
        "        # dump numpy array to disk using pickle\n",
        "        max_bytes = 2**28 - 1 # # 256MB\n",
        "        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(self.PKL_PATH, 'wb') as f_out:\n",
        "            for idx in range(0, len(bytes_out), max_bytes):\n",
        "                f_out.write(bytes_out[idx:idx+max_bytes])\n",
        "        self.logger.debug('words embeddings loaded')\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class GloveEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n",
        "    PKL_PATH = 'gloveembedding.pkl'\n",
        "    N_FEATURES = 50\n",
        "    WORD_MAX_SIZE = 60\n",
        "\n",
        "class FasttextEmbeddings(WordsEmbeddings):\n",
        "    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n",
        "    PKL_PATH = 'fasttext.pkl'\n",
        "    N_FEATURES = 300\n",
        "    WORD_MAX_SIZE = 60"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mosj2-x-kXBK"
      },
      "source": [
        "model_embeddings = GloveEmbeddings()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9FS8ca1ke_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd1d48c-1be3-4f1a-c492-0d3acf376748"
      },
      "source": [
        "print(\"preparing embedding matrix...\")\n",
        "embed_dim = model_embeddings.N_FEATURES\n",
        "words_not_found = []\n",
        "\n",
        "special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
        "for t in special_tokens:\n",
        "    if t not in word2idx_inputs:\n",
        "        word2idx_inputs[t] = len(word2idx_inputs) + 1\n",
        "\n",
        "nb_words = len(word2idx_inputs)\n",
        "embedding_matrix = np.zeros((nb_words + 1, embed_dim))\n",
        "\n",
        "for word, i in word2idx_inputs.items():\n",
        "    if i >= nb_words:\n",
        "        continue\n",
        "\n",
        "    emb_vec = model_embeddings.get_words_embeddings([word])\n",
        "    if emb_vec is not None and len(emb_vec) > 0:\n",
        "        embedding_matrix[i] = emb_vec[0]\n",
        "    else:\n",
        "        words_not_found.append(word)\n",
        "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
        "\n",
        "# asignar tokens especiales expl√≠citamente\n",
        "embedding_matrix[word2idx_inputs[\"<pad>\"]] = np.zeros((embed_dim,))\n",
        "for tok in [\"<unk>\", \"<sos>\", \"<eos>\"]:\n",
        "    embedding_matrix[word2idx_inputs[tok]] = np.random.normal(scale=0.6, size=(embed_dim,))\n",
        "\n",
        "print(f\"number of null word embeddings: {np.sum(np.sum(embedding_matrix, axis=1) == 0)}\")\n",
        "print(f\"coverage: {(1 - np.sum(np.sum(embedding_matrix, axis=1) == 0)/embedding_matrix.shape[0])*100:.2f}%\")\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preparing embedding matrix...\n",
            "number of null word embeddings: 140\n",
            "coverage: 92.24%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vKbhjtIwPgM"
      },
      "source": [
        "### **4 - Entrenar el modelo**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_acc(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Calcula accuracy secuencia a secuencia.\n",
        "    \"\"\"\n",
        "    y_pred_tags = y_pred.argmax(dim=-1)\n",
        "    correct = (y_pred_tags == y_true).float()\n",
        "    acc = correct.sum() / correct.numel()\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train(model, train_loader, valid_loader, optimizer, criterion, epochs=30):\n",
        "    train_loss, train_accuracy = [], []\n",
        "    valid_loss, valid_accuracy = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_train_loss, epoch_train_acc = 0.0, 0.0\n",
        "\n",
        "        for train_encoder_input, train_decoder_input, train_target in train_loader:\n",
        "            # --- mover a GPU ---\n",
        "            train_encoder_input = train_encoder_input.to(device)\n",
        "            train_decoder_input = train_decoder_input.to(device)\n",
        "            train_target = train_target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # --- forward ---\n",
        "            output = model(train_encoder_input, train_decoder_input)  # (batch, seq_len, vocab)\n",
        "\n",
        "            # --- calcular loss (vectorizado) ---\n",
        "            loss = criterion(\n",
        "                output.view(-1, output.shape[-1]),   # (batch*seq, vocab)\n",
        "                train_target.view(-1)                # (batch*seq)\n",
        "            )\n",
        "\n",
        "            # --- backward ---\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # --- metrics ---\n",
        "            epoch_train_loss += loss.item()\n",
        "            epoch_train_acc += sequence_acc(output, train_target).item()\n",
        "\n",
        "        # promedio de epoch\n",
        "        train_loss.append(epoch_train_loss / len(train_loader))\n",
        "        train_accuracy.append(epoch_train_acc / len(train_loader))\n",
        "\n",
        "        # =====================================================\n",
        "        # üîπ VALIDACI√ìN\n",
        "        # =====================================================\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_encoder_input, val_decoder_input, val_target = next(iter(valid_loader))\n",
        "            val_encoder_input = val_encoder_input.to(device)\n",
        "            val_decoder_input = val_decoder_input.to(device)\n",
        "            val_target = val_target.to(device)\n",
        "\n",
        "            val_output = model(val_encoder_input, val_decoder_input)\n",
        "\n",
        "            val_loss = criterion(\n",
        "                val_output.view(-1, val_output.shape[-1]),\n",
        "                val_target.view(-1)\n",
        "            ).item()\n",
        "\n",
        "            val_acc = sequence_acc(val_output, val_target).item()\n",
        "\n",
        "        valid_loss.append(val_loss)\n",
        "        valid_accuracy.append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:02}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss[-1]:.3f} | Train Acc: {train_accuracy[-1]:.3f} | \"\n",
        "              f\"Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.3f}\")\n",
        "\n",
        "    return {\n",
        "        \"loss\": train_loss,\n",
        "        \"accuracy\": train_accuracy,\n",
        "        \"val_loss\": valid_loss,\n",
        "        \"val_accuracy\": valid_accuracy,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "khMYFwFqolTk"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "PAD_TOKEN = word2idx_inputs.get(\"<pad>\", 0)\n",
        "VOCAB_SIZE = embedding_matrix.shape[0]\n",
        "EMBED_DIM = embedding_matrix.shape[1]\n",
        "\n",
        "print(f\"‚úÖ Using device: {device}\")\n",
        "print(f\"Vocab size: {VOCAB_SIZE}, Embedding dim: {EMBED_DIM}\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Definici√≥n de modelos\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_matrix, pad_token, hidden_size=128, num_layers=1, train_embeddings=True):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_matrix.shape[1], padding_idx=pad_token)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = train_embeddings  # desfreezado\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_matrix.shape[1],\n",
        "            hidden_size=hidden_size,\n",
        "            batch_first=True,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        outputs, (h, c) = self.lstm(emb)\n",
        "        return (h, c)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_matrix, pad_token, hidden_size=128, num_layers=1, train_embeddings=True):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_matrix.shape[1], padding_idx=pad_token)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
        "        self.embedding.weight.requires_grad = train_embeddings  # desfreezado\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_matrix.shape[1],\n",
        "            hidden_size=hidden_size,\n",
        "            batch_first=True,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        emb = self.embedding(x)\n",
        "        output, (h, c) = self.lstm(emb, prev_state)\n",
        "        logits = self.fc(output[:, -1, :])\n",
        "        return logits, (h, c)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, encoder_input, decoder_input):\n",
        "        batch_size, seq_len = decoder_input.shape\n",
        "        vocab_size = self.decoder.fc.out_features\n",
        "        outputs = torch.zeros(batch_size, seq_len, vocab_size).to(encoder_input.device)\n",
        "        prev_state = self.encoder(encoder_input)\n",
        "\n",
        "        input_tok = decoder_input[:, 0:1]\n",
        "        for t in range(seq_len):\n",
        "            output, prev_state = self.decoder(input_tok, prev_state)\n",
        "            outputs[:, t, :] = output\n",
        "            input_tok = decoder_input[:, t:t+1]  # teacher forcing\n",
        "        return outputs\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Instanciaci√≥n\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "encoder = Encoder(VOCAB_SIZE, embedding_matrix, PAD_TOKEN)\n",
        "decoder = Decoder(VOCAB_SIZE, embedding_matrix, PAD_TOKEN)\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"‚úÖ Modelo, optimizer y criterion inicializados correctamente\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K82gEEpJ4nUt",
        "outputId": "a547a48c-0508-43dc-994a-8616736397b0"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using device: cuda\n",
            "Vocab size: 1804, Embedding dim: 50\n",
            "‚úÖ Modelo, optimizer y criterion inicializados correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_matrix.shape[0], \"==\", len(word2idx_inputs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prng84mr6lW4",
        "outputId": "25466377-02f4-415e-fba8-1d7940a365ef"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1804 == 1803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_TOKEN = word2idx_inputs.get(\"<pad>\", 0)\n",
        "PAD_TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTgrUs9v6qc_",
        "outputId": "0c37912c-f626-4c74-ffa5-afb06ca67b8a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1800"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2nprd6Pp1TG",
        "outputId": "b5208a8f-b159-4bdc-e542-5805874bde41"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(1804, 50, padding_idx=1800)\n",
              "    (lstm): LSTM(50, 128, batch_first=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(1804, 50, padding_idx=1800)\n",
              "    (lstm): LSTM(50, 128, batch_first=True)\n",
              "    (fc): Linear(in_features=128, out_features=1804, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history1 = train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    valid_loader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    epochs=50\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDB0KWIegt8s",
        "outputId": "42105ba8-c098-46a1-c0ac-edc07bb064b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01/50 | Train Loss: 2.858 | Train Acc: 0.114 | Val Loss: 3.260 | Val Acc: 0.087\n",
            "Epoch 02/50 | Train Loss: 2.644 | Train Acc: 0.120 | Val Loss: 3.078 | Val Acc: 0.095\n",
            "Epoch 03/50 | Train Loss: 2.469 | Train Acc: 0.126 | Val Loss: 2.944 | Val Acc: 0.104\n",
            "Epoch 04/50 | Train Loss: 2.320 | Train Acc: 0.131 | Val Loss: 2.877 | Val Acc: 0.109\n",
            "Epoch 05/50 | Train Loss: 2.184 | Train Acc: 0.134 | Val Loss: 2.816 | Val Acc: 0.113\n",
            "Epoch 06/50 | Train Loss: 2.069 | Train Acc: 0.136 | Val Loss: 2.730 | Val Acc: 0.113\n",
            "Epoch 07/50 | Train Loss: 1.962 | Train Acc: 0.138 | Val Loss: 2.701 | Val Acc: 0.122\n",
            "Epoch 08/50 | Train Loss: 1.869 | Train Acc: 0.141 | Val Loss: 2.634 | Val Acc: 0.122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = range(1, len(history1['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history1['accuracy'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=history1['val_accuracy'], label='valid')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pZzm3tx059Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Modelo con embeddings freezados"
      ],
      "metadata": {
        "id": "swlBRvLJ-U9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(VOCAB_SIZE, embedding_matrix, PAD_TOKEN, train_embeddings=False)\n",
        "decoder = Decoder(VOCAB_SIZE, embedding_matrix, PAD_TOKEN,  train_embeddings=False)\n",
        "model_freezed = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(\"‚úÖ Modelo, optimizer y criterion inicializados correctamente\")\n",
        "\n",
        "history2 = train(\n",
        "    model_freezed,\n",
        "    train_loader,\n",
        "    valid_loader,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "epoch_count = range(1, len(history2['accuracy']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history2['accuracy'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=history2['val_accuracy'], label='valid')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9j2rqeYo-UcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbwn0ekDy_s2"
      },
      "source": [
        "### **5 - Inferencia**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "def infer(model, input_seq, word2idx_output, idx2word_output,\n",
        "          max_output_len=20, device=\"cuda\",\n",
        "          beam_size=3, temperature=1.0, stochastic=False):\n",
        "    \"\"\"\n",
        "    Inferencia con opci√≥n de greedy o beam search.\n",
        "\n",
        "    Args:\n",
        "        model: modelo Seq2Seq (encoder + decoder)\n",
        "        input_seq: secuencia tokenizada de entrada (lista o tensor)\n",
        "        word2idx_output: vocab del decoder (str ‚Üí int)\n",
        "        idx2word_output: vocab inverso (int ‚Üí str)\n",
        "        max_output_len: largo m√°ximo de la secuencia generada\n",
        "        beam_size: cantidad de beams (1 = greedy)\n",
        "        temperature: suaviza distribuci√≥n en modo estoc√°stico\n",
        "        stochastic: si True, elige tokens por muestreo en vez de argmax\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # --- 1Ô∏è‚É£ Codificaci√≥n ---\n",
        "        input_tensor = torch.tensor([input_seq], dtype=torch.long).to(device)\n",
        "        encoder_hidden, encoder_cell = model.encoder(input_tensor)\n",
        "\n",
        "        sos_token = word2idx_output.get(\"<sos>\", 1)\n",
        "        eos_token = word2idx_output.get(\"<eos>\", 2)\n",
        "\n",
        "        # --- 2Ô∏è‚É£ Inicializaci√≥n de beams ---\n",
        "        beams = [{\n",
        "            \"tokens\": [sos_token],\n",
        "            \"hidden\": encoder_hidden,\n",
        "            \"cell\": encoder_cell,\n",
        "            \"logprob\": 0.0\n",
        "        }]\n",
        "\n",
        "        # --- 3Ô∏è‚É£ Decodificaci√≥n autoregresiva ---\n",
        "        for _ in range(max_output_len):\n",
        "            new_beams = []\n",
        "            for beam in beams:\n",
        "                last_token = beam[\"tokens\"][-1]\n",
        "                if last_token == eos_token:\n",
        "                    new_beams.append(beam)\n",
        "                    continue\n",
        "\n",
        "                decoder_input = torch.tensor([[last_token]], dtype=torch.long).to(device)\n",
        "                output, (h, c) = model.decoder(decoder_input, (beam[\"hidden\"], beam[\"cell\"]))\n",
        "\n",
        "                # logits ‚Üí probabilidades\n",
        "                probs = torch.softmax(output.squeeze(0) / temperature, dim=-1).cpu().numpy()\n",
        "\n",
        "                if stochastic:\n",
        "                    # muestreo aleatorio ponderado por softmax\n",
        "                    top_indices = np.random.choice(len(probs), beam_size, p=probs / probs.sum())\n",
        "                else:\n",
        "                    # top-k determin√≠stico\n",
        "                    top_indices = probs.argsort()[-beam_size:][::-1]\n",
        "\n",
        "                for idx in top_indices:\n",
        "                    new_beams.append({\n",
        "                        \"tokens\": beam[\"tokens\"] + [idx],\n",
        "                        \"hidden\": h,\n",
        "                        \"cell\": c,\n",
        "                        \"logprob\": beam[\"logprob\"] + np.log(probs[idx] + 1e-10)\n",
        "                    })\n",
        "\n",
        "            # mantener los mejores beams seg√∫n logprob promedio\n",
        "            beams = sorted(new_beams, key=lambda b: b[\"logprob\"] / len(b[\"tokens\"]), reverse=True)[:beam_size]\n",
        "\n",
        "        # --- 4Ô∏è‚É£ Seleccionar mejor secuencia ---\n",
        "        best_beam = max(beams, key=lambda b: b[\"logprob\"] / len(b[\"tokens\"]))\n",
        "        decoded_tokens = [\n",
        "            tok for tok in best_beam[\"tokens\"]\n",
        "            if tok not in [sos_token, eos_token]\n",
        "        ]\n",
        "\n",
        "        # --- 5Ô∏è‚É£ Decodificar texto ---\n",
        "        decoded_sentence = \" \".join([idx2word_output.get(tok, \"<unk>\") for tok in decoded_tokens])\n",
        "\n",
        "    return decoded_sentence, [b[\"tokens\"] for b in beams]  # devuelve tambi√©n todos los beams\n"
      ],
      "metadata": {
        "id": "P80VW4vTrWMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentence(sentence, tokenizer, max_len):\n",
        "    # limpiar y convertir a √≠ndices usando el tokenizer\n",
        "    seq = tokenizer.texts_to_sequences([sentence.lower().strip()])[0]\n",
        "\n",
        "    # aplicar padding pre para que tenga longitud fija igual a la del entrenamiento\n",
        "    if len(seq) < max_len:\n",
        "        seq = [0] * (max_len - len(seq)) + seq\n",
        "    else:\n",
        "        seq = seq[-max_len:]\n",
        "\n",
        "    return seq\n"
      ],
      "metadata": {
        "id": "xefNxladr6Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_sequences[10]"
      ],
      "metadata": {
        "id": "JXv-bLgh0Pel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentences[10]"
      ],
      "metadata": {
        "id": "K6XCDxHO0g_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> #### Modelo con embeddings entrenables"
      ],
      "metadata": {
        "id": "IxbCkX6q-wTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"i love disney movies \"\n",
        "input_seq = encode_sentence(question, input_tokenizer, max_input_len)\n",
        "\n",
        "generated_answer, beams = infer(\n",
        "    model,\n",
        "    input_seq=input_seq,\n",
        "    word2idx_output=word2idx_outputs,\n",
        "    idx2word_output=idx2word_outputs,\n",
        "    beam_size=5,\n",
        "    temperature=0.8,\n",
        "    stochastic=True,   # False para beam search determin√≠stico\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"Q: {question}\")\n",
        "print(f\"A: {generated_answer}\")\n"
      ],
      "metadata": {
        "id": "e4_IJi-jrWJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> #### Modelo con embeddings freezados"
      ],
      "metadata": {
        "id": "QHZ5WOcx-yry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"i love disney movies \"\n",
        "input_seq = encode_sentence(question, input_tokenizer, max_input_len)\n",
        "\n",
        "generated_answer, beams = infer(\n",
        "    model_freezed,\n",
        "    input_seq=input_seq,\n",
        "    word2idx_output=word2idx_outputs,\n",
        "    idx2word_output=idx2word_outputs,\n",
        "    beam_size=5,\n",
        "    temperature=0.8,\n",
        "    stochastic=True,   # False para beam search determin√≠stico\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"Q: {question}\")\n",
        "print(f\"A: {generated_answer}\")\n"
      ],
      "metadata": {
        "id": "LKU7zAEm-3IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkOjSJweqdF8"
      },
      "source": [
        "### 6 - Conclusi√≥n\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rualuy2NrXDW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}